{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM IMDB Movie Review Tutorial\n",
    "Josiah Olson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import time\n",
    "import numpy as np\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM, Input, merge, BatchNormalization\n",
    "from keras.datasets import imdb\n",
    "\n",
    "import os\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_features = 10000\n",
    "max_len = 200  # cut texts after this number of words (among top max_features most common words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\n",
      "['Bromwell High is a cartoon comedy. It ran at the same time as some other programs about school life, such as \"Teachers\". My 35 years in the teaching profession lead me to believe that Bromwell High\\'s satire is much closer to reality than is \"Teachers\". The scramble to survive financially, the insightful students who can see right through their pathetic teachers\\' pomp, the pettiness of the whole situation, all remind me of the schools I knew and their students. When I saw the episode in which a student repeatedly tried to burn down the school, I immediately recalled ......... at .......... High. A classic line: INSPECTOR: I\\'m here to sack one of your teachers. STUDENT: Welcome to Bromwell High. I expect that many adults of my age think that Bromwell High is far fetched. What a pity that it isn\\'t!']\n",
      "[\"This is one of the dumbest films, I've ever seen. It rips off nearly ever type of thriller and manages to make a mess of them all.<br /><br />There's not a single good line or character in the whole mess. If there was a plot, it was an afterthought and as far as acting goes, there's nothing good to say so Ill say nothing. I honestly cant understand how this type of nonsense gets produced and actually released, does somebody somewhere not at some stage think, 'Oh my god this really is a load of shite' and call it a day. Its crap like this that has people downloading illegally, the trailer looks like a completely different film, at least if you have download it, you haven't wasted your time or money Don't waste your time, this is painful.\"]\n",
      "25000\n",
      "y:\n",
      "[1]\n",
      "[0]\n",
      "25000\n"
     ]
    }
   ],
   "source": [
    "# get dataset and unzip: http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "path = './aclImdb/train/pos/'\n",
    "X_train.extend([open(path + f).read() for f in os.listdir(path) if f.endswith('.txt')])\n",
    "y_train.extend([1 for _ in range(12500)])\n",
    "\n",
    "path = './aclImdb/train/neg/'\n",
    "X_train.extend([open(path + f).read() for f in os.listdir(path) if f.endswith('.txt')])\n",
    "y_train.extend([0 for _ in range(12500)])\n",
    "\n",
    "print('x:')\n",
    "print(X_train[:1])\n",
    "print(X_train[-1:])\n",
    "print(len(X_train))\n",
    "print('y:')\n",
    "print(y_train[:1])\n",
    "print(y_train[-1:])\n",
    "print(len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\n",
      "[\"I went and saw this movie last night after being coaxed to by a few friends of mine. I'll admit that I was reluctant to see it because from what I knew of Ashton Kutcher he was only able to do comedy. I was wrong. Kutcher played the character of Jake Fischer very well, and Kevin Costner played Ben Randall with such professionalism. The sign of a good movie is that it can toy with our emotions. This one did exactly that. The entire theater (which was sold out) was overcome by laughter during the first half of the movie, and were moved to tears during the second half. While exiting the theater I not only saw many women in tears, but many full grown men as well, trying desperately not to let anyone see them crying. This movie was great, and I suggest that you go see it before you judge.\"]\n",
      "['David Bryce\\'s comments nearby are exceptionally well written and informative as almost say everything I feel about DARLING LILI. This massive musical is so peculiar and over blown, over produced and must have caused ruptures at Paramount in 1970. It cost 22 million dollars! That is simply irresponsible. DARLING LILI must have been greenlit from a board meeting that said \"hey we got that Pink Panther guy and that Sound Of Music gal... lets get this too\" and handed over a blank cheque. The result is a hybrid of GIGI, ZEPPELIN, HALF A SIXPENCE, some MGM 40s song and dance numbers of a style (daisies and boaters!) so hopelessly old fashioned as to be like musical porridge, and MATA HARI dramatics. The production is colossal, lush, breathtaking to view, but the rest: the ridiculous romance, Julie looking befuddled, Hudson already dead, the mistimed comedy, and the astoundingly boring songs deaden this spectacular film into being irritating. LILI is like a twee 1940s mega musical with some vulgar bits to spice it up. STAR! released the year before sadly crashed and now is being finally appreciated for the excellent film is genuinely is... and Andrews looks sublime, mature, especially in the last half hour......but LILI is POPPINS and DOLLY frilly and I believe really killed off the mega musical binge of the 60s..... and made Andrews look like Poppins again... which I believe was not Edwards intention. Paramount must have collectively fainted when they saw this: and with another $20 million festering in CATCH 22, and $12 million in ON A CLEAR DAY and $25 million in PAINT YOUR WAGON....they had a financial abyss of CLEOPATRA proportions with $77 million tied into 4 films with very uncertain futures. Maybe they should have asked seer Daisy Gamble from ON A CLEAR DAY ......LILI was very popular on immediate first release in Australia and ran in 70mm cinemas for months but it failed once out in the subs and the sticks and only ever surfaced after that on one night stands with ON A CLEAR DAY as a Sunday night double. Thank god Paramount had their simple $1million (yes, ONE MILLION DOLLAR) film LOVE STORY and that $4 million dollar gangster pic THE GODFATHER also ready to recover all the $77 million in just the next two years....for just $5m.... incredible!']\n",
      "25000\n",
      "y:\n",
      "[1]\n",
      "[0]\n",
      "25000\n"
     ]
    }
   ],
   "source": [
    "# read in the test data\n",
    "\n",
    "X_test = []\n",
    "y_test = []\n",
    "\n",
    "path = './aclImdb/test/pos/'\n",
    "X_test.extend([open(path + f).read() for f in os.listdir(path) if f.endswith('.txt')])\n",
    "y_test.extend([1 for _ in range(12500)])\n",
    "\n",
    "path = './aclImdb/test/neg/'\n",
    "X_test.extend([open(path + f).read() for f in os.listdir(path) if f.endswith('.txt')])\n",
    "y_test.extend([0 for _ in range(12500)])\n",
    "\n",
    "print('x:')\n",
    "print(X_test[:1])\n",
    "print(X_test[-1:])\n",
    "print(len(X_test))\n",
    "print('y:')\n",
    "print(y_test[:1])\n",
    "print(y_test[-1:])\n",
    "print(len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#tokenize works to list of integers where each integer is a key to a word\n",
    "imdbTokenizer = Tokenizer(nb_words=max_features)\n",
    "\n",
    "imdbTokenizer.fit_on_texts(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 with\n",
      "10 i\n",
      "14 as\n",
      "9 it\n",
      "6 is\n",
      "8 in\n",
      "18 but\n",
      "4 of\n",
      "11 this\n",
      "2 and\n",
      "3 a\n",
      "15 for\n",
      "7 br\n",
      "1 the\n",
      "13 was\n",
      "5 to\n",
      "19 film\n",
      "17 movie\n",
      "12 that\n"
     ]
    }
   ],
   "source": [
    "#print top 20 words \n",
    "#note zero is reserved for non frequent words\n",
    "for word, value in imdbTokenizer.word_index.items():\n",
    "    if value < 20:\n",
    "        print(value, word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the\n",
      "and\n",
      "an\n"
     ]
    }
   ],
   "source": [
    "#create int to word dictionary\n",
    "intToWord = {}\n",
    "for word, value in imdbTokenizer.word_index.items():\n",
    "    intToWord[value] = word\n",
    "\n",
    "#add a symbol for null placeholder\n",
    "intToWord[0] = \"!!!NA!!!\"\n",
    "    \n",
    "print(intToWord[1])\n",
    "print(intToWord[2])\n",
    "print(intToWord[32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bromwell High is a cartoon comedy. It ran at the same time as some other programs about school life, such as \"Teachers\". My 35 years in the teaching profession lead me to believe that Bromwell High's satire is much closer to reality than is \"Teachers\". The scramble to survive financially, the insightful students who can see right through their pathetic teachers' pomp, the pettiness of the whole situation, all remind me of the schools I knew and their students. When I saw the episode in which a student repeatedly tried to burn down the school, I immediately recalled ......... at .......... High. A classic line: INSPECTOR: I'm here to sack one of your teachers. STUDENT: Welcome to Bromwell High. I expect that many adults of my age think that Bromwell High is far fetched. What a pity that it isn't!\n",
      "[[309, 6, 3, 1069, 209, 9, 2175, 30, 1, 169, 55, 14, 46, 82, 5869, 41, 393, 110, 138, 14, 5359, 58, 4477, 150, 8, 1, 5032, 5948, 482, 69, 5, 261, 12, 2003, 6, 73, 2436, 5, 632, 71, 6, 5359, 1, 5, 2004, 1, 5941, 1534, 34, 67, 64, 205, 140, 65, 1232, 1, 4, 1, 223, 901, 29, 3024, 69, 4, 1, 5863, 10, 694, 2, 65, 1534, 51, 10, 216, 1, 387, 8, 60, 3, 1472, 3724, 802, 5, 3521, 177, 1, 393, 10, 1238, 30, 309, 3, 353, 344, 2989, 143, 130, 5, 7804, 28, 4, 126, 5359, 1472, 2375, 5, 309, 10, 532, 12, 108, 1470, 4, 58, 556, 101, 12, 309, 6, 227, 4187, 48, 3, 2237, 12, 9, 215]]\n",
      "high\n",
      "is\n",
      "a\n",
      "cartoon\n",
      "comedy\n",
      "it\n",
      "ran\n",
      "at\n",
      "the\n",
      "same\n",
      "time\n",
      "as\n",
      "some\n",
      "other\n",
      "programs\n",
      "about\n",
      "school\n",
      "life\n",
      "such\n",
      "as\n",
      "teachers\n",
      "my\n",
      "35\n",
      "years\n",
      "in\n",
      "the\n",
      "teaching\n",
      "profession\n",
      "lead\n",
      "me\n",
      "to\n",
      "believe\n",
      "that\n",
      "satire\n",
      "is\n",
      "much\n",
      "closer\n",
      "to\n",
      "reality\n",
      "than\n",
      "is\n",
      "teachers\n",
      "the\n",
      "to\n",
      "survive\n",
      "the\n",
      "insightful\n",
      "students\n",
      "who\n",
      "can\n",
      "see\n",
      "right\n",
      "through\n",
      "their\n",
      "pathetic\n",
      "the\n",
      "of\n",
      "the\n",
      "whole\n",
      "situation\n",
      "all\n",
      "remind\n",
      "me\n",
      "of\n",
      "the\n",
      "schools\n",
      "i\n",
      "knew\n",
      "and\n",
      "their\n",
      "students\n",
      "when\n",
      "i\n",
      "saw\n",
      "the\n",
      "episode\n",
      "in\n",
      "which\n",
      "a\n",
      "student\n",
      "repeatedly\n",
      "tried\n",
      "to\n",
      "burn\n",
      "down\n",
      "the\n",
      "school\n",
      "i\n",
      "immediately\n",
      "at\n",
      "high\n",
      "a\n",
      "classic\n",
      "line\n",
      "inspector\n",
      "i'm\n",
      "here\n",
      "to\n",
      "sack\n",
      "one\n",
      "of\n",
      "your\n",
      "teachers\n",
      "student\n",
      "welcome\n",
      "to\n",
      "high\n",
      "i\n",
      "expect\n",
      "that\n",
      "many\n",
      "adults\n",
      "of\n",
      "my\n",
      "age\n",
      "think\n",
      "that\n",
      "high\n",
      "is\n",
      "far\n",
      "fetched\n",
      "what\n",
      "a\n",
      "pity\n",
      "that\n",
      "it\n",
      "isn't\n"
     ]
    }
   ],
   "source": [
    "#convert word strings to integer sequence lists\n",
    "print(X_train[0])\n",
    "print(imdbTokenizer.texts_to_sequences(X_train[:1]))\n",
    "for value in imdbTokenizer.texts_to_sequences(X_train[:1])[0]:\n",
    "    print(intToWord[value])\n",
    "    \n",
    "X_train = imdbTokenizer.texts_to_sequences(X_train)\n",
    "X_test = imdbTokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000 train sequences\n",
      "25000 test sequences\n",
      "Pad sequences (samples x time)\n",
      "X_train shape: (25000, 200)\n",
      "X_test shape: (25000, 200)\n"
     ]
    }
   ],
   "source": [
    "# Censor the data by having a max review length (in number of words)\n",
    "\n",
    "#use this function to load data from keras pickle instead of munging as shown above\n",
    "#(X_train, y_train), (X_test, y_test) = imdb.load_data(nb_words=max_features,\n",
    "#                                                      test_split=0.2)\n",
    "\n",
    "print(len(X_train), 'train sequences')\n",
    "print(len(X_test), 'test sequences')\n",
    "\n",
    "print(\"Pad sequences (samples x time)\")\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_len)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_len)\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('X_test shape:', X_test.shape)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [   0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0  309    6\n",
      "    3 1069  209    9 2175   30    1  169   55   14   46   82 5869   41  393\n",
      "  110  138   14 5359   58 4477  150    8    1 5032 5948  482   69    5  261\n",
      "   12 2003    6   73 2436    5  632   71    6 5359    1    5 2004    1 5941\n",
      " 1534   34   67   64  205  140   65 1232    1    4    1  223  901   29 3024\n",
      "   69    4    1 5863   10  694    2   65 1534   51   10  216    1  387    8\n",
      "   60    3 1472 3724  802    5 3521  177    1  393   10 1238   30  309    3\n",
      "  353  344 2989  143  130    5 7804   28    4  126 5359 1472 2375    5  309\n",
      "   10  532   12  108 1470    4   58  556  101   12  309    6  227 4187   48\n",
      "    3 2237   12    9  215]\n",
      "y: 1\n"
     ]
    }
   ],
   "source": [
    "#example of a sentence sequence, note that lower integers are words that occur more commonly\n",
    "print(\"x:\", X_train[0]) #per observation vector of 20000 words\n",
    "print(\"y:\", y_train[0]) #positive or negative review encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y distribution: (array([0, 1]), array([12500, 12500]))\n",
      "max x word: 9999 ; min x word 0\n",
      "y distribution test: (array([0, 1]), array([12500, 12500]))\n",
      "max x word test: 9999 ; min x word 0\n"
     ]
    }
   ],
   "source": [
    "# double check that word sequences behave/final dimensions are as expected\n",
    "print(\"y distribution:\", np.unique(y_train, return_counts=True))\n",
    "print(\"max x word:\", np.max(X_train), \"; min x word\", np.min(X_train))\n",
    "print(\"y distribution test:\", np.unique(y_test, return_counts=True))\n",
    "print(\"max x word test:\", np.max(X_test), \"; min x word\", np.min(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "most and least popular words: \n",
      "(array([   0,    1,    2, ..., 9997, 9998, 9999], dtype=int32), array([1084310,  232315,  115675, ...,      17,      18,      29]))\n"
     ]
    }
   ],
   "source": [
    "print(\"most and least popular words: \")\n",
    "print(np.unique(X_train, return_counts=True))\n",
    "# as expected zero is the highly used word for words not in index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is an LSTM neural network?\n",
    "\n",
    "A Long Term Short Term Memory Network is a type of Recurrent Neural Network. RNN's have multiple time steps with a feature vecture input at each time step and the prior layer's output/hidden state as an input vector. RNN's come in many varieties, for example, an RNN can take one input and predict multiple outputs, multiple inputs and multiple outputs, multiple inputs to a single output dimension, ext.\n",
    "\n",
    "<img src=\"rnntypes.jpeg\">\n",
    "[source: Karpathy: The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
    "\n",
    "A more specific RNN where one-hot vector of English characters is the input and at character space in the text an input is provided and an output is generated for the predicted next character in the text.\n",
    "\n",
    "<img src=\"charseq.jpeg\">\n",
    "\n",
    "So, how is an RNN different from an LSTM? A normal RNN has the prior output concatenated with the current input to form the feature vector for the current layer. In a basic RNN structure long term dependencies (ie: the subject of a sentence) can be hard for the network to remember over many time steps. This is refered to as the vanishing/exploding gradient problem. More reading is suggested on this, but essentially the non-linearity's effect will compound over time step's causing the prior (historic) gradient to approach either zero or infinity.\n",
    "\n",
    "We provent this problem through the LSTM architecture. LSTM's have an input gate, output gate and forget gate. As simply as possible, this gates control what the output is for the current layer, what part of the past hidden state is forgotten based on the current input, and what part of the current input should be added to the networks long term memory vector which is passed from layer to layer along with the hidden state.\n",
    "\n",
    "<img src=\"lstmchain.png\">\n",
    "[source: Colah: Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#set model hyper parameters\n",
    "epochs = 6\n",
    "embedding_neurons = 128\n",
    "lstm_neurons = 64\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 200)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)          (None, 200, 128)      1280000     input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_1 (BatchNormal(None, 200, 128)      256         embedding_1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                    (None, 64)            49408       batchnormalization_1[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 64)            0           lstm_1[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 1)             65          dropout_1[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 1329729\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Forward Pass LSTM Network\n",
    "\n",
    "# this is the placeholder tensor for the input sequences\n",
    "sequence = Input(shape=(max_len,), dtype='int32')\n",
    "# this embedding layer will transform the sequences of integers\n",
    "# into vectors of size embedding\n",
    "# embedding layer converts dense int input to one-hot in real time to save memory\n",
    "embedded = Embedding(max_features, embedding_neurons, input_length=max_len)(sequence)\n",
    "# normalize embeddings by input/word in sentence\n",
    "bnorm = BatchNormalization()(embedded)\n",
    "\n",
    "# apply forwards LSTM layer size lstm_neurons\n",
    "forwards = LSTM(lstm_neurons, dropout_W=0.2, dropout_U=0.2)(bnorm)\n",
    "\n",
    "# dropout \n",
    "after_dp = Dropout(0.5)(forwards)\n",
    "output = Dense(1, activation='sigmoid')(after_dp)\n",
    "\n",
    "model_fdir_atom = Model(input=sequence, output=output)\n",
    "# review model structure\n",
    "print(model_fdir_atom.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/6\n",
      "251s - loss: 0.5280 - acc: 0.7296 - val_loss: 0.3653 - val_acc: 0.8460\n",
      "Epoch 2/6\n",
      "265s - loss: 0.3151 - acc: 0.8722 - val_loss: 0.3261 - val_acc: 0.8693\n",
      "Epoch 3/6\n",
      "275s - loss: 0.2199 - acc: 0.9150 - val_loss: 0.3573 - val_acc: 0.8603\n",
      "Epoch 4/6\n",
      "265s - loss: 0.1681 - acc: 0.9356 - val_loss: 0.3596 - val_acc: 0.8646\n",
      "Epoch 5/6\n",
      "258s - loss: 0.1237 - acc: 0.9557 - val_loss: 0.4017 - val_acc: 0.8647\n",
      "Epoch 6/6\n",
      "236s - loss: 0.0992 - acc: 0.9645 - val_loss: 0.4646 - val_acc: 0.8651\n",
      "avg sec per epoch: 277.273608486\n"
     ]
    }
   ],
   "source": [
    "# Forward pass LSTM network\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model_fdir_atom.compile('adam', 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "start_time = time.time()\n",
    "\n",
    "history_fdir_atom = model_fdir_atom.fit(X_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    nb_epoch=epochs,\n",
    "                    validation_data=[X_test, y_test], \n",
    "                    verbose=2)\n",
    "\n",
    "end_time = time.time()\n",
    "average_time_per_epoch = (end_time - start_time) / epochs\n",
    "print(\"avg sec per epoch:\", average_time_per_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Bi-directional RNN is a network where the gradient is propagated both forward and backward through time. The hidden state and cell state vector is then the concatenation of the two time steps. This is useful because now the network knows about the input vectors (word embeddings) in front of and behind the current time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_2 (InputLayer)             (None, 200)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)          (None, 200, 128)      1280000     input_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_2 (BatchNormal(None, 200, 128)      256         embedding_2[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                    (None, 64)            49408       batchnormalization_2[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                    (None, 64)            49408       batchnormalization_2[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "merge_1 (Merge)                  (None, 128)           0           lstm_2[0][0]                     \n",
      "                                                                   lstm_3[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)              (None, 128)           0           merge_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 1)             129         dropout_2[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 1379201\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Bi-directional Atom\n",
    "\n",
    "# based on keras tutorial: https://github.com/fchollet/keras/blob/master/examples/imdb_bidirectional_lstm.py\n",
    "\n",
    "# this is the placeholder tensor for the input sequences\n",
    "sequence = Input(shape=(max_len,), dtype='int32')\n",
    "# this embedding layer will transform the sequences of integers\n",
    "# into vectors of size embedding\n",
    "# embedding layer converts dense int input to one-hot in real time to save memory\n",
    "embedded = Embedding(max_features, embedding_neurons, input_length=max_len)(sequence)\n",
    "# normalize embeddings by input/word in sentence\n",
    "bnorm = BatchNormalization()(embedded)\n",
    "\n",
    "# apply forwards LSTM layer size lstm_neurons\n",
    "forwards = LSTM(lstm_neurons, dropout_W=0.4, dropout_U=0.4)(bnorm)\n",
    "# apply backwards LSTM\n",
    "backwards = LSTM(lstm_neurons, dropout_W=0.4, dropout_U=0.4, go_backwards=True)(bnorm)\n",
    "\n",
    "# concatenate the outputs of the 2 LSTMs\n",
    "merged = merge([forwards, backwards], mode='concat', concat_axis=-1)\n",
    "after_dp = Dropout(0.5)(merged)\n",
    "output = Dense(1, activation='sigmoid')(after_dp)\n",
    "\n",
    "model_bidir_atom = Model(input=sequence, output=output)\n",
    "# review model structure\n",
    "print(model_bidir_atom.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/6\n",
      "443s - loss: 0.5878 - acc: 0.6761 - val_loss: 0.3942 - val_acc: 0.8363\n",
      "Epoch 2/6\n",
      "420s - loss: 0.3693 - acc: 0.8443 - val_loss: 0.3311 - val_acc: 0.8598\n",
      "Epoch 3/6\n",
      "398s - loss: 0.2727 - acc: 0.8922 - val_loss: 0.3412 - val_acc: 0.8680\n",
      "Epoch 4/6\n",
      "431s - loss: 0.2175 - acc: 0.9152 - val_loss: 0.3686 - val_acc: 0.8662\n",
      "Epoch 5/6\n",
      "458s - loss: 0.1757 - acc: 0.9345 - val_loss: 0.3819 - val_acc: 0.8677\n",
      "Epoch 6/6\n",
      "465s - loss: 0.1454 - acc: 0.9487 - val_loss: 0.4528 - val_acc: 0.8607\n",
      "avg sec per epoch: 444.341431657\n"
     ]
    }
   ],
   "source": [
    "# Bi-directional Atom\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model_bidir_atom.compile('adam', 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "start_time = time.time()\n",
    "\n",
    "history_bidir_atom = model_bidir_atom.fit(X_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    nb_epoch=epochs,\n",
    "                    validation_data=[X_test, y_test], \n",
    "                    verbose=2)\n",
    "\n",
    "end_time = time.time()\n",
    "average_time_per_epoch = (end_time - start_time) / epochs\n",
    "print(\"avg sec per epoch:\", average_time_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# run simple linear regression to compare performance\n",
    "\n",
    "#based on grid search done by: \n",
    "#https://github.com/rasbt/python-machine-learning-book/blob/master/code/ch08/ch08.ipynb\n",
    "\n",
    "#the tfidf vectors capture co-occurance statistics, think of each number representing how many times \n",
    "#a word occured in a text and scaled by word frequency\n",
    "\n",
    "tfidfTokenizer = Tokenizer(nb_words=max_features)\n",
    "tfidfTokenizer.fit_on_sequences(X_train.tolist())\n",
    "X_train_tfidf = np.asarray(tfidfTokenizer.sequences_to_matrix(X_train.tolist(), mode=\"tfidf\"))\n",
    "X_test_tfidf = np.asarray(tfidfTokenizer.sequences_to_matrix(X_test.tolist(), mode=\"tfidf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.00785793 -3.10109279 -5.29831737 ...,  0.          0.          0.        ]\n",
      " [ 0.         -3.10109279 -4.19970508 ...,  0.          0.          0.        ]\n",
      " [-1.10866262 -2.99573227 -3.91202301 ...,  0.          0.          0.        ]\n",
      " ..., \n",
      " [ 0.         -2.25379493 -3.68887945 ...,  0.          0.          0.        ]\n",
      " [ 0.         -2.73336801 -4.19970508 ...,  0.          0.          0.        ]\n",
      " [-1.10866262 -4.19970508 -3.91202301 ...,  0.          0.          0.        ]]\n",
      "(25000, 10000) (25000, 10000)\n"
     ]
    }
   ],
   "source": [
    "#check tfidf matrix\n",
    "print(X_train_tfidf)\n",
    "print(X_train_tfidf.shape, X_test_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=0, solver='liblinear', tol=0.0001,\n",
       "          verbose=1, warm_start=False)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model_tfidf_reg = LogisticRegression(random_state=0, C=0.001, penalty='l2', verbose=1)\n",
    "model_tfidf_reg.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc: 0.91712\n",
      "test acc: 0.87284\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "#calculate test and train accuracy\n",
    "print(\"train acc:\", accuracy_score(y_test, model_tfidf_reg.predict(X_train_tfidf)))\n",
    "print(\"test acc:\", accuracy_score(y_test, model_tfidf_reg.predict(X_test_tfidf)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What went wrong? Why does our complex neural net not do leaps and bounds better than classic information retrieval techniques? Neural networks tend to excell with lot's of data. If we had around a million movie reviews the literature suggests that the LSTM would vastly outperform the logistic regression. \n",
    "\n",
    "<img src=\"rnnvridge.png\">\n",
    "[source: indico passage github](https://github.com/IndicoDataSolutions/Passage/tree/master/examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_3 (InputLayer)             (None, 200)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)          (None, 200, 128)      1280000     input_3[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_3 (BatchNormal(None, 200, 128)      256         embedding_3[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "lstm_4 (LSTM)                    (None, 64)            49408       batchnormalization_3[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "lstm_5 (LSTM)                    (None, 64)            49408       batchnormalization_3[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "merge_2 (Merge)                  (None, 128)           0           lstm_4[0][0]                     \n",
      "                                                                   lstm_5[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)              (None, 128)           0           merge_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 1)             129         dropout_3[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 1379201\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Bi-directional rmsprop\n",
    "\n",
    "# this example illistrate's that choice of optimizer is an important hyper-parameter for RNNs\n",
    "# rmsprop gives substancially better results than atom\n",
    "# in the literature these two optimizers commonly do well on RNNs\n",
    "\n",
    "# this is the placeholder tensor for the input sequences\n",
    "sequence = Input(shape=(max_len,), dtype='int32')\n",
    "# this embedding layer will transform the sequences of integers\n",
    "# into vectors of size embedding\n",
    "# embedding layer converts dense int input to one-hot in real time to save memory\n",
    "embedded = Embedding(max_features, embedding_neurons, input_length=max_len)(sequence)\n",
    "# normalize embeddings by input/word in sentence\n",
    "bnorm = BatchNormalization()(embedded)\n",
    "\n",
    "# apply forwards LSTM layer size lstm_neurons\n",
    "forwards = LSTM(lstm_neurons, dropout_W=0.4, dropout_U=0.4)(bnorm)\n",
    "# apply backwards LSTM\n",
    "backwards = LSTM(lstm_neurons, dropout_W=0.4, dropout_U=0.4, go_backwards=True)(bnorm)\n",
    "\n",
    "# concatenate the outputs of the 2 LSTMs\n",
    "merged = merge([forwards, backwards], mode='concat', concat_axis=-1)\n",
    "after_dp = Dropout(0.5)(merged)\n",
    "output = Dense(1, activation='sigmoid')(after_dp)\n",
    "\n",
    "model_bidir_rmsprop = Model(input=sequence, output=output)\n",
    "# review model structure\n",
    "print(model_bidir_rmsprop.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/6\n",
      "413s - loss: 0.5825 - acc: 0.6777 - val_loss: 0.4004 - val_acc: 0.8400\n",
      "Epoch 2/6\n",
      "366s - loss: 0.3808 - acc: 0.8373 - val_loss: 0.3620 - val_acc: 0.8598\n",
      "Epoch 3/6\n",
      "375s - loss: 0.2915 - acc: 0.8860 - val_loss: 0.3187 - val_acc: 0.8808\n",
      "Epoch 4/6\n",
      "430s - loss: 0.2440 - acc: 0.9072 - val_loss: 0.3643 - val_acc: 0.8773\n",
      "Epoch 5/6\n",
      "415s - loss: 0.2132 - acc: 0.9206 - val_loss: 0.3616 - val_acc: 0.8785\n",
      "Epoch 6/6\n",
      "421s - loss: 0.1887 - acc: 0.9305 - val_loss: 0.3805 - val_acc: 0.8776\n",
      "avg sec per epoch: 408.525824308\n"
     ]
    }
   ],
   "source": [
    "# Bi-directional rmsprop\n",
    "\n",
    "model_bidir_rmsprop.compile('rmsprop', 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "start_time = time.time()\n",
    "\n",
    "history_bidir_rmsprop = model_bidir_rmsprop.fit(X_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    nb_epoch=epochs,\n",
    "                    validation_data=[X_test, y_test], \n",
    "                    verbose=2)\n",
    "\n",
    "end_time = time.time()\n",
    "average_time_per_epoch = (end_time - start_time) / epochs\n",
    "print(\"avg sec per epoch:\", average_time_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'W_constraint': None, 'activity_regularizer': None, 'name': 'embedding_3', 'output_dim': 128, 'trainable': True, 'init': 'uniform', 'input_dtype': 'int32', 'mask_zero': False, 'batch_input_shape': (None, 200), 'W_regularizer': None, 'dropout': 0.0, 'input_dim': 10000, 'input_length': 200}\n",
      "(10000, 128)\n"
     ]
    }
   ],
   "source": [
    "#get weights from embedding layer and visualize\n",
    "\n",
    "print(model_bidir_rmsprop.layers[1].get_config())\n",
    "embmatrix = model_bidir_rmsprop.layers[1].get_weights()[0]\n",
    "print(embmatrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 2)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "topnwords = 5000\n",
    "toptsne = TSNE(n_components=2, random_state=0)\n",
    "tsneXY = toptsne.fit_transform(embmatrix[:topnwords, :]) \n",
    "tsneXY.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "displaytopnwords = 100\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(tsneXY[:displaytopnwords, 0], tsneXY[:displaytopnwords, 1])\n",
    "\n",
    "for i in range(displaytopnwords):\n",
    "    ax.annotate(intToWord[i], (tsneXY[i, 0], tsneXY[i, 1]))\n",
    "\n",
    "fig.set_size_inches(25, 25)\n",
    "plt.show()\n",
    "# notice that great, most, well are clustered\n",
    "# bad don't even are clustered\n",
    "# We've learned structure in our sentiment embedding\n",
    "# neural networks give us this and other useful features for free"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note in the image above that 'script' is close to 'bad', apparently more people use that in the negative context. Also, 'performance' is close to 'him', it appears that the embedding is learning a reference back to the pronoun.\n",
    "Note, how far apart the clusters of positive and negative words are, negative words at the top of the TSNE embedding. Another cool observation, 'music' is close to 'feel', again think that aspect of this embedding is cinema specific."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-1.0574172639269411e+23, 9.2707202694120441e+23, '!!!NA!!!')\n",
      "(6.7943275034306018e+22, 2.4466575226236019e+23, 'the')\n",
      "(8.4183317116045373e+22, 1.437326900840428e+24, 'and')\n",
      "(1.9285223868655347e+22, 7.5660905394261448e+22, 'a')\n",
      "(1.9084665792533479e+23, 1.7170613415369986e+24, 'of')\n",
      "(4.9875226842070545e+23, 4.2337549069800415e+24, 'to')\n",
      "(1.1226071781241927e+24, 9.2530730526642411e+23, 'is')\n",
      "(2.0957396617298205e+23, 1.1112016492929392e+24, 'br')\n",
      "(-3.4520217114767206e+23, 9.2603793445198658e+23, 'in')\n",
      "(1.6297537539031633e+23, 1.5985913698648461e+23, 'it')\n",
      "(-1.2677679835710844e+25, -2.1332779191894091e+25, 'i')\n",
      "(2.4500573468924788e+23, 6.117232757292037e+23, 'this')\n",
      "(1.7720665333425818e+24, 3.0048020812946899e+24, 'that')\n",
      "(2.0049640136053522e+22, 2.8235204448706629e+24, 'was')\n",
      "(2.5044823283342875e+23, 2.5020851287655714e+24, 'as')\n",
      "(-1.7286829083088888e+23, -5.3316478932726971e+22, 'for')\n",
      "(2.2393600225286364e+23, 2.9319996048975886e+24, 'with')\n",
      "(2.2324039145506218e+23, 2.0034742189756343e+24, 'movie')\n",
      "(4.350274137717044e+23, 2.7382683211597528e+24, 'but')\n",
      "(1.8268858691291574e+24, -2.296641940332749e+24, 'film')\n",
      "(9.6633231351096132e+23, 5.7304217596129156e+23, 'on')\n",
      "(-7.4484257148354544e+21, -8.7759457088792573e+22, 'not')\n",
      "(1.4947274812271388e+23, -1.834485089798879e+24, 'you')\n",
      "(2.1266844710947858e+23, -2.8592457821649918e+23, 'are')\n",
      "(7.286778594793695e+22, -1.1578238825999017e+24, 'his')\n",
      "(3.6070903938447106e+23, 1.1145171274201759e+24, 'have')\n",
      "(-1.2198737936676244e+24, -1.3911925503205177e+25, 'he')\n",
      "(-1.2021559802673443e+23, -4.2875785514551354e+23, 'be')\n",
      "(2.4684471221039341e+23, 6.5255886942851316e+23, 'one')\n",
      "(8.5980223508617663e+22, 4.4701234460870584e+23, 'all')\n",
      "(3.1486912329587312e+22, -2.6323785656575749e+23, 'at')\n",
      "(1.3552490529504684e+23, 6.6687423275833239e+24, 'by')\n",
      "(3.0743041027568721e+23, -9.1439107508004905e+24, 'an')\n",
      "(2.6044706019009887e+23, 7.3560279022217579e+23, 'they')\n",
      "(7.2027737030733269e+23, 1.5750800890345728e+24, 'who')\n",
      "(-8.6416388650718639e+23, -2.3060591674555704e+23, 'so')\n",
      "(7.6640657820650341e+22, 2.6895130808165866e+24, 'from')\n",
      "(-9.3044846482760879e+23, -3.4630438761731871e+24, 'like')\n",
      "(1.4315604718530757e+24, 3.5072198455699669e+24, 'her')\n",
      "(-3.2991164115885182e+23, 6.6534551719094455e+24, 'or')\n",
      "(-5.5453570410059615e+23, 2.0276298211052709e+24, 'just')\n",
      "(-1.03699720103398e+23, 9.5545174186583896e+22, 'about')\n",
      "(3.9976296489450911e+23, 7.2265231960702952e+23, \"it's\")\n",
      "(3.782645287158447e+22, 4.0958006451617598e+23, 'out')\n",
      "(8.9546765060301809e+22, 9.6290444263710167e+23, 'has')\n",
      "(-2.6392837755344668e+22, 6.1294956142671743e+23, 'if')\n",
      "(-4.4752284213281492e+22, 3.5509512498601661e+23, 'some')\n",
      "(1.0696602052919008e+23, 5.1827893822177933e+23, 'there')\n",
      "(-1.1940743974945708e+24, -1.8313461587881129e+24, 'what')\n",
      "(3.5389970190175465e+23, 1.1944373716338652e+24, 'good')\n",
      "(2.0005511437474935e+22, 7.7702251134809025e+22, 'more')\n",
      "(4.2981834691213516e+23, 2.2559630514362451e+24, 'when')\n",
      "(-2.8537286265007166e+23, -1.5577673506733707e+23, 'very')\n",
      "(2.6357835114003207e+23, 1.1364694722851877e+24, 'up')\n",
      "(3.2480958862840315e+23, 2.9431371449301709e+24, 'no')\n",
      "(1.4304509010926427e+23, 4.6883504977475604e+23, 'time')\n",
      "(-1.597893708313272e+23, -1.1835349952867644e+23, 'she')\n",
      "(1.1193448635479745e+23, 7.866985218001413e+23, 'even')\n",
      "(1.661091092081267e+23, 1.2677070660988126e+24, 'my')\n",
      "(3.4463339770089482e+23, 1.011731938280694e+24, 'would')\n",
      "(5.2343913241243348e+22, 8.5782039027813563e+23, 'which')\n",
      "(-1.0373284754788742e+23, 1.5527261600827625e+24, 'only')\n",
      "(-9.2058886116515142e+23, -2.3608938235210946e+24, 'story')\n",
      "(3.9757638082763366e+23, 2.6137163242257873e+24, 'really')\n",
      "(7.0221855672442252e+23, 1.3588059777898808e+24, 'see')\n",
      "(9.0328199498365932e+22, -3.6846808463779174e+23, 'their')\n",
      "(-3.8784069057220583e+23, -1.6727487413230311e+25, 'had')\n",
      "(-7.6424896737482864e+22, 2.8125335806180802e+23, 'can')\n",
      "(2.764224999989208e+23, 3.6696903752786673e+24, 'were')\n",
      "(1.0828811334937597e+23, 6.9954092899121018e+23, 'me')\n",
      "(1.6509034060488984e+23, -3.6857483674723746e+23, 'well')\n",
      "(-8.116865539506029e+23, -1.0162742888118926e+24, 'than')\n",
      "(-1.7631308025036886e+24, 2.6601903983947811e+24, 'we')\n",
      "(8.2630659430056328e+22, 7.6234913032621148e+23, 'much')\n",
      "(3.780284201828246e+23, 1.0868738939835146e+24, 'been')\n",
      "(7.0815672303923677e+22, 3.8243591846363753e+23, 'bad')\n",
      "(1.651165262826052e+24, -1.1278602050084768e+25, 'get')\n",
      "(7.7700502550293846e+22, -1.532996241096412e+23, 'will')\n",
      "(8.8233046029874345e+23, 4.025182888745868e+24, 'do')\n",
      "(2.7840165170173719e+23, -1.0624082137412174e+23, 'also')\n",
      "(5.0713565161054797e+23, 3.3619596941500755e+24, 'into')\n",
      "(3.4017325720580493e+23, 2.9813783704794416e+24, 'people')\n",
      "(-1.7190111686167611e+23, -4.6973056078019905e+23, 'other')\n",
      "(8.4796100198590648e+22, 6.7016206536885001e+23, 'first')\n",
      "(-1.7745638094726609e+23, -3.4069967268468195e+23, 'great')\n",
      "(6.0047334939463432e+22, 4.9753059340198006e+23, 'because')\n",
      "(9.576870171163778e+22, 2.0990238551382295e+23, 'how')\n",
      "(2.4344133562128204e+23, -8.3127876607810529e+23, 'him')\n",
      "(-1.1258612028490936e+23, 2.4617670670042431e+23, 'most')\n",
      "(3.1351531777108344e+23, 7.3263990772672217e+23, \"don't\")\n",
      "(1.3640668486919335e+23, 6.7458655636720864e+23, 'made')\n",
      "(1.7508759557955418e+24, 2.6415547798507749e+25, 'its')\n",
      "(-6.0733867877318053e+22, 1.2673290655674316e+24, 'then')\n",
      "(2.3655406250036027e+23, 4.537682861005759e+24, 'way')\n",
      "(2.129613643832778e+23, 1.5463678534946312e+24, 'make')\n",
      "(5.8387172022497411e+23, 2.7137530506444514e+24, 'them')\n",
      "(1.4033845954279071e+23, 4.5981025547006597e+24, 'too')\n",
      "(-7.8615906347433889e+22, 9.4009986321166202e+23, 'could')\n",
      "(-5.1401775396530986e+22, 2.6236659909264617e+24, 'any')\n",
      "(2.4038430410220291e+23, 4.9342977166066142e+23, 'movies')\n",
      "(-2.0790902526702277e+23, 7.4242427134097934e+23, 'after')\n",
      "(7.5858871006142035e+23, 3.31452390105841e+24, 'think')\n",
      "(6.3940191914827134e+22, 3.9129486253947595e+23, 'characters')\n",
      "(1.6346295781401196e+24, 7.7295682897737468e+24, 'watch')\n",
      "(1.7504008865794196e+24, 2.3415427100647786e+24, 'two')\n",
      "(-1.5187097051944102e+23, 1.320824975376711e+24, 'films')\n",
      "(6.6597324374179469e+22, 1.5282265741805791e+24, 'character')\n",
      "(3.5761157613053507e+23, 5.5694766802160835e+24, 'seen')\n",
      "(-2.4037803986518952e+23, 6.3338928600102364e+23, 'many')\n",
      "(1.6494113955071468e+23, 1.1894002757465402e+24, 'being')\n",
      "(1.8938486660966786e+23, 5.3232219770131023e+23, 'life')\n",
      "(1.0691188999353077e+20, 1.8370994471199782e+24, 'plot')\n",
      "(-2.0317764784063657e+22, -3.3204781365686708e+24, 'never')\n",
      "(-5.6239997114397809e+23, 1.294462806691351e+24, 'acting')\n",
      "(3.5500437427117539e+23, 1.6665257702031994e+24, 'little')\n",
      "(-2.1223516543038513e+23, 6.5862187512641029e+23, 'best')\n",
      "(-1.2353750286885534e+23, 1.8542623193787791e+24, 'love')\n",
      "(1.1444358932689369e+23, 6.0182035496071257e+23, 'over')\n",
      "(-9.939012826667103e+20, 2.2825980777970591e+23, 'where')\n",
      "(-3.4345225123584765e+22, 3.3603157112112617e+23, 'did')\n",
      "(4.2886345536287477e+23, 1.6818120062700849e+24, 'show')\n",
      "(8.7989578582400738e+22, 2.6502972189266555e+23, 'know')\n",
      "(-1.8315074713311083e+23, 2.8162381996327354e+23, 'off')\n",
      "(-1.1093238977213415e+24, 1.431787382972385e+24, 'ever')\n",
      "(-1.8974679972982686e+23, -6.1435290403231879e+23, 'does')\n",
      "(3.0674073322100632e+23, 5.3497327994822212e+23, 'better')\n",
      "(1.5968240496752817e+23, 8.4281542507372004e+23, 'your')\n",
      "(5.9729295613791438e+22, 4.5168945356431013e+23, 'end')\n",
      "(-7.7238995861352091e+22, -3.1535626597221827e+23, 'still')\n",
      "(8.4147130598497356e+22, 1.7278577673699136e+24, 'man')\n",
      "(5.887938322737217e+22, 2.9808150650686283e+23, 'here')\n",
      "(5.9827604738450644e+23, 1.8863545392041437e+24, 'these')\n",
      "(5.3542786518520668e+22, -2.4137578005462224e+23, 'say')\n",
      "(1.2180414481527527e+23, 1.0591821299421545e+24, 'scene')\n",
      "(1.0203771043917937e+23, 8.036757103353777e+23, 'while')\n",
      "(1.1493804236692694e+23, 1.6071355917705255e+24, 'why')\n",
      "(4.9264442305916e+22, 6.9373150360811201e+23, 'scenes')\n",
      "(-8.0032150833016215e+21, -3.9231337440174661e+23, 'go')\n",
      "(5.4736682480622294e+23, 3.8649658940501817e+24, 'such')\n",
      "(2.7151440936968533e+23, 1.9376322376141776e+24, 'something')\n",
      "(-3.0634280610834277e+23, -2.551887465801003e+24, 'through')\n",
      "(7.5204765334022472e+22, 6.9318526998822065e+23, 'should')\n",
      "(1.4159975992688532e+23, 2.2113205560262689e+23, 'back')\n",
      "(1.5806901781165967e+23, 1.6913786313018805e+24, \"i'm\")\n",
      "(-4.138153001892023e+22, -1.0166435308334447e+24, 'real')\n",
      "(-4.5250237449516036e+23, -7.6533672813008148e+23, 'those')\n",
      "(5.6292903885255385e+23, -8.2520413918286392e+24, 'watching')\n",
      "(-5.5324785308982848e+22, -1.2393828074801514e+24, 'now')\n",
      "(-1.2344530571701494e+24, -3.6551293805634066e+24, 'though')\n",
      "(2.2092597351104782e+23, 2.3607464729950286e+24, \"doesn't\")\n",
      "(3.7033904744338013e+23, 6.8450484837104957e+23, 'years')\n",
      "(1.9202102797522467e+23, 5.2363150827621817e+22, 'old')\n",
      "(2.6082391385159838e+23, 8.171122034814046e+23, 'thing')\n",
      "(-5.7667406413503285e+23, -2.7224742558335368e+23, 'actors')\n",
      "(2.8878425841147887e+22, 5.3684100156408927e+23, 'work')\n",
      "(-4.9524649140237736e+22, 1.3646924239170535e+23, '10')\n",
      "(8.5329652706375098e+22, -1.2997297616806467e+23, 'before')\n",
      "(3.5294387931341092e+22, -2.6642212419000905e+22, 'another')\n",
      "(1.5433666849499026e+23, 5.7840879109510846e+23, \"didn't\")\n",
      "(-1.8209960502077284e+23, 1.2478837329200203e+24, 'new')\n",
      "(1.0520579053862676e+23, 5.6516778869817974e+23, 'funny')\n",
      "(1.9919253697283596e+25, 1.4794830826688004e+25, 'nothing')\n",
      "(5.8435265777563211e+21, 2.1166007642246527e+24, 'actually')\n",
      "(-3.75698372758648e+23, 2.1755274559248992e+23, 'makes')\n",
      "(3.4351401629456108e+22, 1.2841291238404446e+24, 'director')\n",
      "(3.6141650431586752e+23, -9.1893609673771805e+23, 'look')\n",
      "(-3.0698014591629841e+23, 1.0362741707398594e+24, 'find')\n",
      "(-1.9338831791363175e+23, 3.2323374483703103e+23, 'going')\n",
      "(1.7296980755090652e+23, 3.8532591080563845e+23, 'few')\n",
      "(-5.5889116877671071e+22, 7.5285467031558995e+23, 'same')\n",
      "(-4.3011968974381702e+22, 3.0871698075255524e+23, 'part')\n",
      "(1.9156131884586089e+22, 4.3983046424892694e+23, 'again')\n",
      "(5.0490698216912342e+22, 3.289432559813964e+23, 'every')\n",
      "(2.8701988798109452e+23, 1.2444266560864766e+24, 'lot')\n",
      "(3.325020968411174e+23, 7.6160163972582976e+23, 'cast')\n",
      "(2.143910937876858e+23, 2.3776955745848846e+24, 'us')\n",
      "(4.7223340254575305e+24, 2.1100874184581526e+24, 'quite')\n",
      "(1.852906256929337e+24, 1.3285496450099925e+24, 'down')\n",
      "(2.0033355504716716e+22, 1.7095101912114368e+24, 'want')\n",
      "(2.0638149565086581e+23, 1.3372579950956098e+24, 'world')\n",
      "(1.7329453892552992e+23, 1.219534149991267e+23, 'things')\n",
      "(5.0652856613617534e+23, 5.9653484296823132e+23, 'pretty')\n",
      "(3.1884933029739538e+22, 3.6725527530309012e+23, 'young')\n",
      "(5.8297691491444334e+22, 2.621351789401054e+22, 'seems')\n",
      "(1.8983623748608107e+23, 1.7025155468634995e+24, 'around')\n",
      "(1.5071585267267971e+23, 9.243764659666817e+23, 'got')\n",
      "(2.1153258390124396e+23, 2.1630537809953603e+23, 'horror')\n",
      "(-5.3811395268406908e+23, 2.4333440973301137e+24, 'however')\n",
      "(1.4975414152102451e+23, 1.4566087693067927e+24, \"can't\")\n",
      "(5.9921215645495864e+22, 5.8827538056531912e+23, 'fact')\n",
      "(2.3564257707591648e+23, 2.5303563265230185e+24, 'take')\n",
      "(2.0234195081268329e+23, 6.2867982001973516e+24, 'big')\n",
      "(2.65463770195492e+22, 1.1591134569661822e+23, 'enough')\n",
      "(6.2659689761420595e+22, 9.1864351966267077e+23, 'long')\n",
      "(2.8157732684357619e+23, 2.8085236168955479e+24, 'thought')\n",
      "(7.2472228901137823e+22, 8.0965256360192163e+23, \"that's\")\n",
      "(3.3146434022872671e+23, 3.7643326160230989e+24, 'both')\n",
      "(5.4608825548441947e+23, 5.5711862795887225e+23, 'between')\n",
      "(4.8652053032378266e+23, 1.9740091387399486e+24, 'series')\n",
      "(-3.9731681317361533e+23, -4.0458293945909318e+22, 'give')\n",
      "(9.1368910637725733e+23, 2.5384486517708916e+24, 'may')\n",
      "(-1.4342592444153098e+23, 4.3654023385533294e+23, 'original')\n",
      "(2.1208719355443701e+24, -4.2404212692657247e+24, 'own')\n",
      "(3.3272766656452559e+23, 3.6288270906372354e+24, 'action')\n",
      "(5.9000466664893777e+23, 3.4488552609974572e+24, \"i've\")\n",
      "(1.3891504045518042e+23, -2.2633712504204597e+23, 'right')\n",
      "(1.6860224775180668e+23, 7.6415703306627894e+23, 'without')\n",
      "(4.6161620607764713e+23, 9.7815776540656501e+23, 'always')\n",
      "(1.8757088934502427e+23, 1.1159349139580418e+24, 'times')\n",
      "(-7.8161577415223633e+23, -3.3306818569067063e+24, 'comedy')\n",
      "(-1.2117459162595932e+25, -3.6063052211736045e+24, 'point')\n",
      "(4.5855398978387132e+23, 1.1817480983172592e+24, 'gets')\n",
      "(9.310851306832214e+22, 6.159842628244986e+23, 'must')\n",
      "(7.5459578511179888e+22, 2.6145625134483795e+24, 'come')\n",
      "(-6.0347158777198372e+24, -2.1076654755862945e+25, 'role')\n",
      "(2.5915766952654261e+23, 8.0361115135690754e+23, \"isn't\")\n",
      "(9.8134007412723496e+23, 2.2437437373025041e+24, 'saw')\n",
      "(6.9018398870384579e+23, 1.935469572525573e+24, 'almost')\n",
      "(-1.3628859638106439e+22, 2.6648858946666496e+24, 'interesting')\n",
      "(-7.7270253204102641e+23, 1.3162036785814264e+24, 'least')\n",
      "(7.1347849980947357e+22, 1.802604991423097e+24, 'family')\n",
      "(2.9378797298668621e+23, 3.6831311813430557e+24, 'done')\n",
      "(1.8682237061303809e+24, 6.9096175253555358e+24, \"there's\")\n",
      "(1.3359322882376925e+23, 2.3596816682611229e+24, 'whole')\n",
      "(6.2113947556801185e+22, 1.0587485669487369e+24, 'bit')\n",
      "(1.6785047320034876e+23, 3.1240464341440167e+23, 'music')\n",
      "(4.9458621682599907e+23, 9.2474881665913866e+23, 'script')\n",
      "(-1.6860241614120529e+23, 2.5956136719698309e+24, 'far')\n",
      "(3.5915920728943251e+23, 9.8683385140388194e+22, 'making')\n",
      "(-4.4944927663771143e+24, 7.9328157194372209e+24, 'guy')\n",
      "(1.4324354763753328e+23, 6.5976738634495812e+23, 'anything')\n",
      "(5.0721423172855866e+23, 2.9148682454851402e+22, 'minutes')\n",
      "(5.7297606920618715e+22, 2.3527723259416789e+23, 'feel')\n",
      "(-5.7513924096063777e+23, -2.1886183631759445e+24, 'last')\n",
      "(6.8518247974609661e+22, 3.1768433625001797e+23, 'since')\n",
      "(2.093967603676808e+24, 6.9576653145796523e+23, 'might')\n",
      "(2.4311469338677828e+23, 1.2947608945362571e+24, 'performance')\n",
      "(8.1002631414983755e+22, 3.2352577367700163e+22, \"he's\")\n",
      "(4.8620609071711099e+22, 1.1589516229945542e+24, '2')\n",
      "(6.6081310083727685e+23, 2.1740315443791211e+24, 'probably')\n",
      "(3.8058788647572831e+22, -8.7114417779061262e+23, 'kind')\n",
      "(-5.6115464115956102e+22, 3.4507674266515845e+23, 'am')\n",
      "(1.5062453293037046e+23, -7.3505474340576448e+23, 'away')\n",
      "(1.8240428743667764e+23, 2.6316843405684278e+24, 'yet')\n",
      "(5.1451278883199422e+23, 4.6130968066656892e+24, 'rather')\n",
      "(1.5757879773943541e+22, 1.2263760760277455e+24, 'tv')\n",
      "(2.9428269692303998e+22, -3.3759799240015092e+24, 'worst')\n",
      "(-4.1545266862958275e+23, -2.0332382974619973e+24, 'girl')\n",
      "(4.2392838545489911e+22, 4.2387624729623356e+23, 'day')\n",
      "(9.7821315089040954e+22, 8.3215426495528651e+23, 'sure')\n"
     ]
    }
   ],
   "source": [
    "#guide to chart above\n",
    "\n",
    "for i in range(displaytopnwords):\n",
    "    print((tsneXY[i, 0], tsneXY[i, 1], intToWord[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('great', 0.0)\n",
      "('outstanding', 0.5220314264297485)\n",
      "('fantastic', 0.5242985486984253)\n",
      "('tears', 0.5253304243087769)\n",
      "('provoking', 0.528724193572998)\n",
      "('subtitles', 0.5348644852638245)\n",
      "('satisfying', 0.5392454266548157)\n",
      "('packed', 0.5427441000938416)\n",
      "('intense', 0.5463260412216187)\n",
      "('succeeds', 0.5507190227508545)\n",
      "('prince', 0.5507473349571228)\n",
      "('balance', 0.5522542595863342)\n",
      "('closed', 0.5536043047904968)\n",
      "('searching', 0.5538246035575867)\n",
      "('worlds', 0.5542515516281128)\n",
      "('everyday', 0.5542992949485779)\n",
      "('haunting', 0.5561468005180359)\n",
      "('gently', 0.5562131404876709)\n",
      "('impact', 0.5564712285995483)\n",
      "('touching', 0.5576462149620056)\n"
     ]
    }
   ],
   "source": [
    "# Lets see what the embedding learned, \n",
    "# provoking is close to great in cosine space, that's cool and definettly movie specific\n",
    "\n",
    "from scipy.spatial.distance import euclidean\n",
    "\n",
    "for value in np.argsort(np.apply_along_axis(lambda x: euclidean(x, embmatrix[imdbTokenizer.word_index['great'],:]), \n",
    "                                            1, embmatrix))[:20]:\n",
    "    print((intToWord[value], euclidean(embmatrix[value,:], embmatrix[imdbTokenizer.word_index['great'],:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('great', -5.3852722725622471e-08)\n",
      "('fantastic', 0.24784848571634588)\n",
      "('provoking', 0.25040505358361731)\n",
      "('outstanding', 0.25513591509504019)\n",
      "('satisfying', 0.25696442062953584)\n",
      "('wonderful', 0.26944760167646931)\n",
      "('haunting', 0.27100288204200151)\n",
      "('tears', 0.27722195331209343)\n",
      "('touching', 0.27855866196148393)\n",
      "('worlds', 0.27965967293686145)\n",
      "('perfectly', 0.28751167688020829)\n",
      "('subtle', 0.28810029859446495)\n",
      "('intense', 0.29067268778187227)\n",
      "('impact', 0.29237831152734395)\n",
      "('excellent', 0.29283647530093904)\n",
      "('captures', 0.29304984435225001)\n",
      "('witty', 0.29534486555998929)\n",
      "('subtitles', 0.29604452894134059)\n",
      "('perfect', 0.29651742030751982)\n",
      "('vhs', 0.29705147009207933)\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "for value in np.argsort(np.apply_along_axis(lambda x: cosine(x, embmatrix[imdbTokenizer.word_index['great'],:]), \n",
    "                                            1, embmatrix))[:20]:\n",
    "    print((intToWord[value], cosine(embmatrix[value,:], embmatrix[imdbTokenizer.word_index['great'],:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdbTokenizer.word_index['great']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 200, 1)\n"
     ]
    }
   ],
   "source": [
    "# Prep the data to do a prediction at each time step\n",
    "# goal is to use this model to figure out what words cause predicted sentiment to change\n",
    "# reshape to predict at every time step the review sentiment\n",
    "\n",
    "y_train_multi = np.repeat(y_train.reshape((-1,1)), max_len, axis=1).reshape((-1,max_len,1))\n",
    "y_test_multi = np.repeat(y_test.reshape((-1,1)), max_len, axis=1).reshape((-1,max_len,1))\n",
    "print(y_train_multi.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_4 (InputLayer)             (None, 200)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)          (None, 200, 128)      1280000     input_4[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_4 (BatchNormal(None, 200, 128)      256         embedding_4[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "lstm_6 (LSTM)                    (None, 200, 64)       49408       batchnormalization_4[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)              (None, 200, 64)       0           lstm_6[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "timedistributed_1 (TimeDistribute(None, 200, 1)        65          dropout_4[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 1329729\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Forward Pass LSTM Network multi step predict\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "\n",
    "# this is the placeholder tensor for the input sequences\n",
    "sequence = Input(shape=(max_len,), dtype='int32')\n",
    "# this embedding layer will transform the sequences of integers\n",
    "# into vectors of size embedding\n",
    "# embedding layer converts dense int input to one-hot in real time to save memory\n",
    "embedded = Embedding(max_features, embedding_neurons, input_length=max_len)(sequence)\n",
    "# normalize embeddings by input/word in sentence\n",
    "bnorm = BatchNormalization()(embedded)\n",
    "\n",
    "# apply forwards LSTM layer size lstm_neurons\n",
    "forwards = LSTM(lstm_neurons, dropout_W=0.2, dropout_U=0.2, return_sequences=True)(bnorm)\n",
    "\n",
    "# dropout \n",
    "after_dp = Dropout(0.5)(forwards)\n",
    "output = TimeDistributed(Dense(1, activation='sigmoid'))(after_dp)\n",
    "\n",
    "model_fdir_multi = Model(input=sequence, output=output)\n",
    "# review model structure\n",
    "print(model_fdir_multi.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/6\n",
      "265s - loss: 0.5722 - acc: 0.6808 - val_loss: 0.5181 - val_acc: 0.7210\n",
      "Epoch 2/6\n",
      "264s - loss: 0.4743 - acc: 0.7461 - val_loss: 0.5151 - val_acc: 0.7188\n",
      "Epoch 3/6\n",
      "263s - loss: 0.4384 - acc: 0.7637 - val_loss: 0.5386 - val_acc: 0.7242\n",
      "Epoch 4/6\n",
      "247s - loss: 0.4149 - acc: 0.7755 - val_loss: 0.5264 - val_acc: 0.7147\n",
      "Epoch 5/6\n",
      "254s - loss: 0.3979 - acc: 0.7826 - val_loss: 0.5730 - val_acc: 0.7214\n",
      "Epoch 6/6\n",
      "237s - loss: 0.3821 - acc: 0.7917 - val_loss: 0.5689 - val_acc: 0.7144\n",
      "avg sec per epoch: 260.834505836\n"
     ]
    }
   ],
   "source": [
    "# Forward pass LSTM network multi step predict\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model_fdir_multi.compile('rmsprop', 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "start_time = time.time()\n",
    "\n",
    "history_fdir_multi = model_fdir_multi.fit(X_train, y_train_multi,\n",
    "                    batch_size=batch_size,\n",
    "                    nb_epoch=epochs,\n",
    "                    validation_data=[X_test, y_test_multi], \n",
    "                    verbose=2)\n",
    "\n",
    "end_time = time.time()\n",
    "average_time_per_epoch = (end_time - start_time) / epochs\n",
    "print(\"avg sec per epoch:\", average_time_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 200, 1)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_pred_mult = model_fdir_multi.predict(X_test)\n",
    "y_test_pred_mult.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.86360000000000003"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# as a sanity check look at the accuracy of the final prediction\n",
    "accuracy_score(y_test_multi[:,-1,:].ravel(), y_test_pred_mult[:,-1,:].ravel() > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg starting review: 0.494475334883\n",
      "max starting review: 0.873785555363\n",
      "min starting review: 0.108750365674\n"
     ]
    }
   ],
   "source": [
    "print(\"avg starting review:\", np.median(y_test_pred_mult[:,0,:]))\n",
    "print(\"max starting review:\", np.max(y_test_pred_mult[:,0,:]))\n",
    "print(\"min starting review:\", np.min(y_test_pred_mult[:,0,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.51862627782892434"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(y_test_pred_mult[:,-1,:].ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.49447533]\n",
      "[ 0.4967725]\n",
      "[ 0.49767217]\n",
      "[ 0.49793971]\n",
      "[ 0.49800527]\n",
      "[ 0.49801993]\n",
      "[ 0.49802265]\n",
      "[ 0.49802276]\n",
      "[ 0.49802247]\n",
      "[ 0.49802229]\n",
      "[ 0.49802229]\n",
      "[ 0.49802241]\n",
      "[ 0.49802256]\n",
      "[ 0.49802276]\n",
      "[ 0.49802294]\n",
      "[ 0.49802309]\n",
      "[ 0.49802321]\n",
      "[ 0.49802336]\n",
      "[ 0.49802345]\n",
      "[ 0.49802351]\n",
      "[ 0.49802357]\n",
      "[ 0.49802363]\n",
      "[ 0.49802366]\n",
      "[ 0.49802369]\n",
      "[ 0.49802372]\n",
      "[ 0.49802375]\n",
      "[ 0.49802375]\n",
      "[ 0.49802375]\n",
      "[ 0.49802375]\n",
      "[ 0.49802378]\n",
      "[ 0.49802378]\n",
      "[ 0.49802378]\n",
      "[ 0.49802378]\n",
      "[ 0.49802378]\n",
      "[ 0.49802378]\n",
      "[ 0.49802378]\n",
      "[ 0.49802378]\n",
      "[ 0.49802378]\n",
      "[ 0.49802378]\n",
      "[ 0.49802378]\n",
      "[ 0.49802381]\n",
      "[ 0.49802378]\n",
      "[ 0.49802381]\n",
      "[ 0.49802378]\n",
      "[ 0.49802381]\n",
      "[ 0.49802378]\n",
      "[ 0.49802381]\n",
      "[ 0.49802378]\n",
      "[ 0.49802381]\n",
      "[ 0.49802378]\n",
      "[ 0.49802381]\n",
      "[ 0.49802378]\n",
      "[ 0.49802381]\n",
      "[ 0.49802378]\n",
      "[ 0.49802381]\n",
      "[ 0.49802378]\n",
      "[ 0.47142845]\n",
      "[ 0.40759853]\n",
      "[ 0.42302248]\n",
      "[ 0.50436586]\n",
      "[ 0.50741732]\n",
      "[ 0.5609141]\n",
      "[ 0.53248161]\n",
      "[ 0.50584793]\n",
      "[ 0.51162189]\n",
      "[ 0.48714817]\n",
      "[ 0.57271308]\n",
      "[ 0.53836042]\n",
      "[ 0.54877466]\n",
      "[ 0.50153649]\n",
      "[ 0.58411759]\n",
      "[ 0.58784729]\n",
      "[ 0.69875133]\n",
      "[ 0.62782139]\n",
      "[ 0.65890825]\n",
      "[ 0.70554483]\n",
      "[ 0.67135453]\n",
      "[ 0.69561344]\n",
      "[ 0.67530346]\n",
      "[ 0.71391892]\n",
      "[ 0.76563859]\n",
      "[ 0.73088771]\n",
      "[ 0.7507565]\n",
      "[ 0.76601917]\n",
      "[ 0.74750078]\n",
      "[ 0.73869157]\n",
      "[ 0.69414109]\n",
      "[ 0.76857805]\n",
      "[ 0.78863531]\n",
      "[ 0.79299569]\n",
      "[ 0.76289499]\n",
      "[ 0.78345251]\n",
      "[ 0.82089144]\n",
      "[ 0.83465892]\n",
      "[ 0.84138131]\n",
      "[ 0.83390403]\n",
      "[ 0.84199423]\n",
      "[ 0.80514538]\n",
      "[ 0.81390476]\n",
      "[ 0.80044019]\n",
      "[ 0.78879559]\n",
      "[ 0.78414118]\n",
      "[ 0.88029575]\n",
      "[ 0.85344243]\n",
      "[ 0.86119372]\n",
      "[ 0.892151]\n",
      "[ 0.89926732]\n",
      "[ 0.90550882]\n",
      "[ 0.93964803]\n",
      "[ 0.94973469]\n",
      "[ 0.96009952]\n",
      "[ 0.96285403]\n",
      "[ 0.95218849]\n",
      "[ 0.96221864]\n",
      "[ 0.95337689]\n",
      "[ 0.96710795]\n",
      "[ 0.96359032]\n",
      "[ 0.96233308]\n",
      "[ 0.9637177]\n",
      "[ 0.96397835]\n",
      "[ 0.96313101]\n",
      "[ 0.96995771]\n",
      "[ 0.97063851]\n",
      "[ 0.95990568]\n",
      "[ 0.96853143]\n",
      "[ 0.98458803]\n",
      "[ 0.97105843]\n",
      "[ 0.97785538]\n",
      "[ 0.96581769]\n",
      "[ 0.96608901]\n",
      "[ 0.97141629]\n",
      "[ 0.97294939]\n",
      "[ 0.95602345]\n",
      "[ 0.95866317]\n",
      "[ 0.9601894]\n",
      "[ 0.95626885]\n",
      "[ 0.96792197]\n",
      "[ 0.96012896]\n",
      "[ 0.95973539]\n",
      "[ 0.97526032]\n",
      "[ 0.97075665]\n",
      "[ 0.97571987]\n",
      "[ 0.97358322]\n",
      "[ 0.96921551]\n",
      "[ 0.97471339]\n",
      "[ 0.95307958]\n",
      "[ 0.94897699]\n",
      "[ 0.95616871]\n",
      "[ 0.95694417]\n",
      "[ 0.96423197]\n",
      "[ 0.97111541]\n",
      "[ 0.97348315]\n",
      "[ 0.97494763]\n",
      "[ 0.97849035]\n",
      "[ 0.97472477]\n",
      "[ 0.97184038]\n",
      "[ 0.96507442]\n",
      "[ 0.94223225]\n",
      "[ 0.95712]\n",
      "[ 0.95865631]\n",
      "[ 0.9591291]\n",
      "[ 0.96238035]\n",
      "[ 0.95516217]\n",
      "[ 0.95563143]\n",
      "[ 0.95289183]\n",
      "[ 0.95730406]\n",
      "[ 0.96369642]\n",
      "[ 0.96029329]\n",
      "[ 0.97586131]\n",
      "[ 0.9652158]\n",
      "[ 0.96114939]\n",
      "[ 0.95752162]\n",
      "[ 0.97167969]\n",
      "[ 0.96832806]\n",
      "[ 0.97128433]\n",
      "[ 0.96797049]\n",
      "[ 0.96122789]\n",
      "[ 0.9297058]\n",
      "[ 0.9343847]\n",
      "[ 0.92897826]\n",
      "[ 0.91465408]\n",
      "[ 0.92494959]\n",
      "[ 0.93416774]\n",
      "[ 0.9441126]\n",
      "[ 0.93812633]\n",
      "[ 0.94235963]\n",
      "[ 0.93915457]\n",
      "[ 0.94184667]\n",
      "[ 0.95527875]\n",
      "[ 0.95862287]\n",
      "[ 0.96901089]\n",
      "[ 0.96375245]\n",
      "[ 0.95899308]\n",
      "[ 0.96746308]\n",
      "[ 0.96510857]\n",
      "[ 0.96602255]\n",
      "[ 0.9584167]\n",
      "[ 0.96340597]\n",
      "[ 0.96978301]\n",
      "[ 0.9718098]\n",
      "[ 0.65615976]\n",
      "[ 0.78804594]\n",
      "[ 0.83730125]\n",
      "[ 0.8861081]\n",
      "[ 0.86096013]\n",
      "[ 0.8544147]\n",
      "[ 0.89893574]\n",
      "[ 0.92543977]\n",
      "[ 0.92270058]\n",
      "[ 0.96902943]\n",
      "[ 0.96449149]\n",
      "[ 0.98011684]\n",
      "[ 0.97161573]\n",
      "[ 0.9706918]\n",
      "[ 0.96928549]\n",
      "[ 0.96972179]\n",
      "[ 0.98027748]\n",
      "[ 0.9742505]\n",
      "[ 0.97956324]\n",
      "[ 0.97454667]\n",
      "[ 0.97386062]\n",
      "[ 0.97704011]\n",
      "[ 0.97096974]\n",
      "[ 0.97329211]\n",
      "[ 0.97198439]\n",
      "[ 0.97808272]\n",
      "[ 0.97542936]\n",
      "[ 0.97909182]\n",
      "[ 0.9723773]\n",
      "[ 0.98629075]\n",
      "[ 0.98170781]\n",
      "[ 0.98464704]\n",
      "[ 0.98448998]\n",
      "[ 0.98092186]\n",
      "[ 0.98065019]\n",
      "[ 0.98068726]\n",
      "[ 0.97482812]\n",
      "[ 0.9851861]\n",
      "[ 0.98524821]\n",
      "[ 0.97927856]\n",
      "[ 0.97939092]\n",
      "[ 0.98016441]\n",
      "[ 0.98209226]\n",
      "[ 0.98243016]\n",
      "[ 0.98155862]\n",
      "[ 0.97855049]\n",
      "[ 0.98115468]\n",
      "[ 0.97854888]\n",
      "[ 0.9767136]\n",
      "[ 0.97925311]\n",
      "[ 0.98096114]\n",
      "[ 0.98319364]\n",
      "[ 0.98289937]\n",
      "[ 0.98233145]\n",
      "[ 0.98116702]\n",
      "[ 0.98497021]\n",
      "[ 0.9845289]\n",
      "[ 0.98199868]\n",
      "[ 0.98065335]\n",
      "[ 0.9780581]\n",
      "[ 0.98127609]\n",
      "[ 0.9842844]\n",
      "[ 0.97925168]\n",
      "[ 0.985309]\n",
      "[ 0.98242873]\n",
      "[ 0.98403442]\n",
      "[ 0.98380226]\n",
      "[ 0.98335892]\n",
      "[ 0.97396439]\n",
      "[ 0.98165327]\n",
      "[ 0.9810884]\n",
      "[ 0.98275512]\n",
      "[ 0.98160368]\n",
      "[ 0.9841345]\n",
      "[ 0.97855943]\n",
      "[ 0.98401105]\n",
      "[ 0.97848874]\n",
      "[ 0.98304611]\n",
      "[ 0.98633122]\n",
      "[ 0.98188841]\n",
      "[ 0.98417407]\n",
      "[ 0.98372459]\n",
      "[ 0.98244941]\n",
      "[ 0.9812538]\n",
      "[ 0.97980732]\n",
      "[ 0.97753775]\n",
      "[ 0.97950208]\n",
      "[ 0.97168803]\n",
      "[ 0.97513437]\n",
      "[ 0.96536702]\n",
      "[ 0.94467974]\n",
      "[ 0.97249717]\n",
      "[ 0.97600067]\n",
      "[ 0.9764266]\n",
      "[ 0.98364204]\n",
      "[ 0.97800666]\n",
      "[ 0.97855288]\n",
      "[ 0.98247474]\n",
      "[ 0.97883886]\n",
      "[ 0.97924781]\n",
      "[ 0.98048377]\n",
      "[ 0.97380769]\n",
      "[ 0.97898328]\n",
      "[ 0.97821379]\n",
      "[ 0.97853065]\n",
      "[ 0.97814596]\n",
      "[ 0.98289692]\n",
      "[ 0.98062116]\n",
      "[ 0.98215616]\n",
      "[ 0.97963846]\n",
      "[ 0.98414642]\n",
      "[ 0.98630369]\n",
      "[ 0.98172706]\n",
      "[ 0.9836539]\n",
      "[ 0.97927678]\n",
      "[ 0.98251873]\n",
      "[ 0.98071486]\n",
      "[ 0.98516643]\n",
      "[ 0.97957504]\n",
      "[ 0.98024166]\n",
      "[ 0.98523092]\n",
      "[ 0.9774043]\n",
      "[ 0.98083949]\n",
      "[ 0.98518962]\n",
      "[ 0.98294467]\n",
      "[ 0.98322493]\n",
      "[ 0.98462087]\n",
      "[ 0.98369652]\n",
      "[ 0.98261434]\n",
      "[ 0.97756124]\n",
      "[ 0.98089266]\n",
      "[ 0.9833799]\n",
      "[ 0.98416018]\n",
      "[ 0.9833945]\n",
      "[ 0.97773325]\n",
      "[ 0.97647828]\n",
      "[ 0.97108412]\n",
      "[ 0.9763186]\n",
      "[ 0.97884762]\n",
      "[ 0.98197955]\n",
      "[ 0.9829011]\n",
      "[ 0.98080373]\n",
      "[ 0.98240912]\n",
      "[ 0.97533184]\n",
      "[ 0.97839904]\n",
      "[ 0.97769517]\n",
      "[ 0.98035949]\n",
      "[ 0.97601527]\n",
      "[ 0.97373354]\n",
      "[ 0.97943777]\n",
      "[ 0.98365486]\n",
      "[ 0.98163545]\n",
      "[ 0.98327547]\n",
      "[ 0.98491162]\n",
      "[ 0.98144162]\n",
      "[ 0.98451358]\n",
      "[ 0.97645128]\n",
      "[ 0.98453104]\n",
      "[ 0.985237]\n",
      "[ 0.98342389]\n",
      "[ 0.98502851]\n",
      "[ 0.98148179]\n",
      "[ 0.98180318]\n",
      "[ 0.98184657]\n",
      "[ 0.98400348]\n",
      "[ 0.98177069]\n",
      "[ 0.9824428]\n",
      "[ 0.98417574]\n",
      "[ 0.98469371]\n",
      "[ 0.97959584]\n",
      "[ 0.98088145]\n",
      "[ 0.97685772]\n",
      "[ 0.9802556]\n",
      "[ 0.98197806]\n",
      "[ 0.98554641]\n",
      "[ 0.98209232]\n",
      "[ 0.98056901]\n",
      "[ 0.97881424]\n",
      "[ 0.97769493]\n",
      "[ 0.9767496]\n",
      "[ 0.98130399]\n",
      "[ 0.98130029]\n",
      "[ 0.98159355]\n",
      "[ 0.98262715]\n",
      "[ 0.97667587]\n",
      "[ 0.97988427]\n",
      "[ 0.98312628]\n",
      "[ 0.98001415]\n",
      "[ 0.98328453]\n",
      "[ 0.98399568]\n",
      "[ 0.98630106]\n",
      "[ 0.98482996]\n",
      "[ 0.98403484]\n",
      "[ 0.98436105]\n",
      "[ 0.97872895]\n",
      "[ 0.98457569]\n",
      "[ 0.98544055]\n",
      "[ 0.98550647]\n",
      "[ 0.97919965]\n",
      "[ 0.98164082]\n",
      "[ 0.50441307]\n",
      "[ 0.49123204]\n",
      "[ 0.47546405]\n",
      "[ 0.5055027]\n",
      "[ 0.56274539]\n",
      "[ 0.49092436]\n",
      "[ 0.6173678]\n",
      "[ 0.60009104]\n",
      "[ 0.66197795]\n",
      "[ 0.61837357]\n",
      "[ 0.73895729]\n",
      "[ 0.71447957]\n",
      "[ 0.70198739]\n",
      "[ 0.72076231]\n",
      "[ 0.75135005]\n",
      "[ 0.74395788]\n",
      "[ 0.78949553]\n",
      "[ 0.69950503]\n",
      "[ 0.70304126]\n",
      "[ 0.71974427]\n",
      "[ 0.76007754]\n",
      "[ 0.77614301]\n",
      "[ 0.71476686]\n",
      "[ 0.66547257]\n",
      "[ 0.66085446]\n",
      "[ 0.640567]\n",
      "[ 0.64131242]\n",
      "[ 0.65934002]\n",
      "[ 0.63393533]\n",
      "[ 0.65431345]\n",
      "[ 0.61177403]\n",
      "[ 0.65286928]\n",
      "[ 0.69283271]\n",
      "[ 0.51942945]\n",
      "[ 0.52784187]\n",
      "[ 0.5593093]\n",
      "[ 0.55382389]\n",
      "[ 0.52831441]\n",
      "[ 0.54520506]\n",
      "[ 0.54757529]\n",
      "[ 0.58238614]\n",
      "[ 0.58981037]\n",
      "[ 0.59963876]\n",
      "[ 0.58016735]\n",
      "[ 0.60677302]\n",
      "[ 0.57728708]\n",
      "[ 0.61083066]\n",
      "[ 0.62605751]\n",
      "[ 0.65368348]\n",
      "[ 0.6501919]\n",
      "[ 0.67791277]\n",
      "[ 0.66426408]\n",
      "[ 0.67747241]\n",
      "[ 0.7446577]\n",
      "[ 0.74355286]\n",
      "[ 0.72869092]\n",
      "[ 0.6569792]\n",
      "[ 0.55898643]\n",
      "[ 0.56855768]\n",
      "[ 0.54099113]\n",
      "[ 0.62203068]\n",
      "[ 0.61887497]\n",
      "[ 0.54249769]\n",
      "[ 0.54923069]\n",
      "[ 0.53911924]\n",
      "[ 0.53995317]\n",
      "[ 0.53397453]\n",
      "[ 0.77463663]\n",
      "[ 0.86125159]\n",
      "[ 0.77038628]\n",
      "[ 0.7796436]\n",
      "[ 0.77314043]\n",
      "[ 0.74854684]\n",
      "[ 0.81871092]\n",
      "[ 0.83629805]\n",
      "[ 0.83018529]\n",
      "[ 0.85776132]\n",
      "[ 0.89496011]\n",
      "[ 0.87798637]\n",
      "[ 0.90675092]\n",
      "[ 0.89163697]\n",
      "[ 0.96064198]\n",
      "[ 0.95934027]\n",
      "[ 0.96209383]\n",
      "[ 0.96575075]\n",
      "[ 0.97113955]\n",
      "[ 0.96764046]\n",
      "[ 0.96313661]\n",
      "[ 0.9736557]\n",
      "[ 0.97475529]\n",
      "[ 0.97555387]\n",
      "[ 0.96368217]\n",
      "[ 0.95866251]\n",
      "[ 0.95730424]\n",
      "[ 0.96984118]\n",
      "[ 0.95923918]\n",
      "[ 0.95911402]\n",
      "[ 0.9489125]\n",
      "[ 0.9455992]\n",
      "[ 0.92822629]\n",
      "[ 0.93717349]\n",
      "[ 0.940781]\n",
      "[ 0.93413955]\n",
      "[ 0.90999532]\n",
      "[ 0.93827546]\n",
      "[ 0.92276734]\n",
      "[ 0.91712219]\n",
      "[ 0.91587579]\n",
      "[ 0.95716524]\n",
      "[ 0.95273435]\n",
      "[ 0.96124738]\n",
      "[ 0.97210914]\n",
      "[ 0.96864235]\n",
      "[ 0.97097671]\n",
      "[ 0.97000313]\n",
      "[ 0.96962172]\n",
      "[ 0.97039694]\n",
      "[ 0.96955907]\n",
      "[ 0.97422123]\n",
      "[ 0.9703396]\n",
      "[ 0.97905576]\n",
      "[ 0.97587842]\n",
      "[ 0.97715825]\n",
      "[ 0.97992206]\n",
      "[ 0.9822191]\n",
      "[ 0.98020935]\n",
      "[ 0.98025393]\n",
      "[ 0.98107988]\n",
      "[ 0.97865367]\n",
      "[ 0.95497859]\n",
      "[ 0.96307498]\n",
      "[ 0.96042377]\n",
      "[ 0.96633208]\n",
      "[ 0.96790785]\n",
      "[ 0.96985704]\n",
      "[ 0.96935731]\n",
      "[ 0.97630221]\n",
      "[ 0.97690141]\n",
      "[ 0.97247583]\n",
      "[ 0.97025675]\n",
      "[ 0.97576785]\n",
      "[ 0.97690541]\n",
      "[ 0.96491778]\n",
      "[ 0.95822489]\n",
      "[ 0.96579021]\n",
      "[ 0.96287084]\n",
      "[ 0.95641971]\n",
      "[ 0.95905536]\n",
      "[ 0.96054453]\n",
      "[ 0.9619996]\n",
      "[ 0.96609563]\n",
      "[ 0.96100545]\n",
      "[ 0.9803912]\n",
      "[ 0.97164255]\n",
      "[ 0.97961169]\n",
      "[ 0.97284347]\n",
      "[ 0.9693591]\n",
      "[ 0.98059452]\n",
      "[ 0.97304767]\n",
      "[ 0.97244185]\n",
      "[ 0.97859329]\n",
      "[ 0.97120708]\n",
      "[ 0.97280979]\n",
      "[ 0.98130155]\n",
      "[ 0.9721269]\n",
      "[ 0.97516167]\n",
      "[ 0.97661579]\n",
      "[ 0.98096412]\n",
      "[ 0.98120701]\n",
      "[ 0.97563016]\n",
      "[ 0.97745621]\n",
      "[ 0.9807542]\n",
      "[ 0.98396707]\n",
      "[ 0.97453922]\n",
      "[ 0.98160499]\n",
      "[ 0.98452616]\n",
      "[ 0.98181069]\n",
      "[ 0.97479296]\n",
      "[ 0.9626658]\n",
      "[ 0.97563243]\n",
      "[ 0.97973454]\n",
      "[ 0.97470206]\n",
      "[ 0.9776212]\n",
      "[ 0.97608268]\n",
      "[ 0.97778589]\n",
      "[ 0.97716743]\n",
      "[ 0.98298168]\n",
      "[ 0.97334689]\n",
      "[ 0.96715337]\n",
      "[ 0.97875643]\n",
      "[ 0.97498649]\n",
      "[ 0.97593051]\n",
      "[ 0.97769797]\n",
      "[ 0.9747473]\n",
      "[ 0.97107863]\n",
      "[ 0.97406566]\n",
      "[ 0.97936064]\n",
      "[ 0.96968865]\n",
      "[ 0.9629904]\n",
      "[ 0.97561949]\n",
      "[ 0.49447533]\n",
      "[ 0.4967725]\n",
      "[ 0.49767217]\n",
      "[ 0.49793971]\n",
      "[ 0.49800527]\n",
      "[ 0.49801993]\n",
      "[ 0.49802265]\n",
      "[ 0.49802276]\n",
      "[ 0.49802247]\n",
      "[ 0.49802229]\n",
      "[ 0.49802229]\n",
      "[ 0.49802241]\n",
      "[ 0.49802256]\n",
      "[ 0.49802276]\n",
      "[ 0.49802294]\n",
      "[ 0.49802309]\n",
      "[ 0.49802321]\n",
      "[ 0.49802336]\n",
      "[ 0.49802345]\n",
      "[ 0.49802351]\n",
      "[ 0.49802357]\n",
      "[ 0.49802363]\n",
      "[ 0.49802366]\n",
      "[ 0.49802369]\n",
      "[ 0.49802372]\n",
      "[ 0.49802375]\n",
      "[ 0.49802375]\n",
      "[ 0.49802375]\n",
      "[ 0.49802375]\n",
      "[ 0.49802378]\n",
      "[ 0.49802378]\n",
      "[ 0.49802378]\n",
      "[ 0.49802378]\n",
      "[ 0.49802378]\n",
      "[ 0.49802378]\n",
      "[ 0.49802378]\n",
      "[ 0.49802378]\n",
      "[ 0.49802378]\n",
      "[ 0.49802378]\n",
      "[ 0.49802378]\n",
      "[ 0.49802381]\n",
      "[ 0.49802378]\n",
      "[ 0.49802381]\n",
      "[ 0.49802378]\n",
      "[ 0.49802381]\n",
      "[ 0.49802378]\n",
      "[ 0.49802381]\n",
      "[ 0.49802378]\n",
      "[ 0.49802381]\n",
      "[ 0.49802378]\n",
      "[ 0.49802381]\n",
      "[ 0.49802378]\n",
      "[ 0.49802381]\n",
      "[ 0.49802378]\n",
      "[ 0.49802381]\n",
      "[ 0.49802378]\n",
      "[ 0.49802381]\n",
      "[ 0.49802378]\n",
      "[ 0.49802381]\n",
      "[ 0.49802378]\n",
      "[ 0.49802381]\n",
      "[ 0.49802378]\n",
      "[ 0.49802381]\n",
      "[ 0.49802378]\n",
      "[ 0.49802381]\n",
      "[ 0.49802378]\n",
      "[ 0.49802381]\n",
      "[ 0.49802378]\n",
      "[ 0.49802381]\n",
      "[ 0.49802378]\n",
      "[ 0.49802381]\n",
      "[ 0.49802378]\n",
      "[ 0.49802381]\n",
      "[ 0.49802378]\n",
      "[ 0.49802381]\n",
      "[ 0.49802378]\n",
      "[ 0.49802381]\n",
      "[ 0.47142845]\n",
      "[ 0.57311755]\n",
      "[ 0.52097255]\n",
      "[ 0.62599111]\n",
      "[ 0.63323808]\n",
      "[ 0.65919918]\n",
      "[ 0.43637475]\n",
      "[ 0.38641369]\n",
      "[ 0.33695546]\n",
      "[ 0.35200843]\n",
      "[ 0.40681207]\n",
      "[ 0.81972849]\n",
      "[ 0.83252156]\n",
      "[ 0.86148906]\n",
      "[ 0.83036464]\n",
      "[ 0.88872331]\n",
      "[ 0.9385848]\n",
      "[ 0.92340213]\n",
      "[ 0.94187182]\n",
      "[ 0.92531383]\n",
      "[ 0.94302434]\n",
      "[ 0.95349377]\n",
      "[ 0.93689853]\n",
      "[ 0.94287574]\n",
      "[ 0.9313916]\n",
      "[ 0.96712577]\n",
      "[ 0.97463971]\n",
      "[ 0.96032715]\n",
      "[ 0.9661535]\n",
      "[ 0.96532989]\n",
      "[ 0.95769954]\n",
      "[ 0.95449758]\n",
      "[ 0.96162385]\n",
      "[ 0.96293372]\n",
      "[ 0.96768862]\n",
      "[ 0.97042149]\n",
      "[ 0.96565032]\n",
      "[ 0.96750587]\n",
      "[ 0.96914583]\n",
      "[ 0.97634625]\n",
      "[ 0.97538137]\n",
      "[ 0.97600448]\n",
      "[ 0.97124565]\n",
      "[ 0.9787823]\n",
      "[ 0.9766227]\n",
      "[ 0.97299916]\n",
      "[ 0.98191798]\n",
      "[ 0.97798133]\n",
      "[ 0.98529154]\n",
      "[ 0.97871816]\n",
      "[ 0.97547233]\n",
      "[ 0.97457343]\n",
      "[ 0.97699541]\n",
      "[ 0.98081923]\n",
      "[ 0.98009104]\n",
      "[ 0.98053008]\n",
      "[ 0.97200757]\n",
      "[ 0.9836306]\n",
      "[ 0.98099518]\n",
      "[ 0.98186618]\n",
      "[ 0.98410869]\n",
      "[ 0.97981328]\n",
      "[ 0.97888607]\n",
      "[ 0.98015302]\n",
      "[ 0.98594481]\n",
      "[ 0.98465377]\n",
      "[ 0.98372692]\n",
      "[ 0.98181051]\n",
      "[ 0.97784901]\n",
      "[ 0.98519182]\n",
      "[ 0.98140377]\n",
      "[ 0.98310977]\n",
      "[ 0.98365229]\n",
      "[ 0.98296511]\n",
      "[ 0.98170304]\n",
      "[ 0.9829334]\n",
      "[ 0.98280019]\n",
      "[ 0.98043942]\n",
      "[ 0.98510063]\n",
      "[ 0.97865438]\n",
      "[ 0.9843958]\n",
      "[ 0.98110843]\n",
      "[ 0.98120242]\n",
      "[ 0.98286849]\n",
      "[ 0.9809739]\n",
      "[ 0.98671079]\n",
      "[ 0.98363388]\n",
      "[ 0.97788632]\n",
      "[ 0.97946703]\n",
      "[ 0.97573882]\n",
      "[ 0.97897524]\n",
      "[ 0.98551816]\n",
      "[ 0.98424137]\n",
      "[ 0.98216224]\n",
      "[ 0.98209089]\n",
      "[ 0.98423976]\n",
      "[ 0.97789609]\n",
      "[ 0.98245734]\n",
      "[ 0.98502403]\n",
      "[ 0.98304206]\n",
      "[ 0.98561454]\n",
      "[ 0.97393388]\n",
      "[ 0.98505694]\n",
      "[ 0.98163646]\n",
      "[ 0.97966301]\n",
      "[ 0.98446739]\n",
      "[ 0.97135049]\n",
      "[ 0.97237825]\n",
      "[ 0.96373963]\n",
      "[ 0.97268248]\n",
      "[ 0.96975917]\n",
      "[ 0.97578329]\n",
      "[ 0.98391503]\n",
      "[ 0.97519356]\n",
      "[ 0.98077607]\n",
      "[ 0.97924757]\n",
      "[ 0.97265404]\n",
      "[ 0.97473562]\n",
      "[ 0.97756338]\n",
      "[ 0.98343736]\n",
      "[ 0.97973609]\n",
      "[ 0.98281473]\n",
      "[ 0.98378229]\n",
      "[ 0.49447533]\n",
      "[ 0.4967725]\n",
      "[ 0.49767217]\n",
      "[ 0.49793971]\n",
      "[ 0.49800527]\n",
      "[ 0.56236839]\n",
      "[ 0.80706346]\n",
      "[ 0.82048541]\n",
      "[ 0.9188661]\n",
      "[ 0.90731382]\n",
      "[ 0.91534621]\n",
      "[ 0.92725056]\n",
      "[ 0.91587913]\n",
      "[ 0.91399479]\n",
      "[ 0.90477532]\n",
      "[ 0.82716995]\n",
      "[ 0.84512872]\n",
      "[ 0.81044668]\n",
      "[ 0.84581077]\n",
      "[ 0.88837743]\n",
      "[ 0.87540966]\n",
      "[ 0.87817526]\n",
      "[ 0.87295502]\n",
      "[ 0.88594574]\n",
      "[ 0.86108965]\n",
      "[ 0.87920702]\n",
      "[ 0.87926]\n",
      "[ 0.85365146]\n",
      "[ 0.88537735]\n",
      "[ 0.91520536]\n",
      "[ 0.94052309]\n",
      "[ 0.93093723]\n",
      "[ 0.85102141]\n",
      "[ 0.81456858]\n",
      "[ 0.78319049]\n",
      "[ 0.82412785]\n",
      "[ 0.87166697]\n",
      "[ 0.86869633]\n",
      "[ 0.84182525]\n",
      "[ 0.84165549]\n",
      "[ 0.84291613]\n",
      "[ 0.89509022]\n",
      "[ 0.87593281]\n",
      "[ 0.87875891]\n",
      "[ 0.86440426]\n",
      "[ 0.92215574]\n",
      "[ 0.92794472]\n",
      "[ 0.95929831]\n",
      "[ 0.94962746]\n",
      "[ 0.95086646]\n",
      "[ 0.97248393]\n",
      "[ 0.96341705]\n",
      "[ 0.93919891]\n",
      "[ 0.95669353]\n",
      "[ 0.94091922]\n",
      "[ 0.9610554]\n",
      "[ 0.95057815]\n",
      "[ 0.96534091]\n",
      "[ 0.9544214]\n",
      "[ 0.95592636]\n",
      "[ 0.96270883]\n",
      "[ 0.95575047]\n",
      "[ 0.97326106]\n",
      "[ 0.96416736]\n",
      "[ 0.97448379]\n",
      "[ 0.97191763]\n",
      "[ 0.97539318]\n",
      "[ 0.96446741]\n",
      "[ 0.98209035]\n",
      "[ 0.97067529]\n",
      "[ 0.96635473]\n",
      "[ 0.98069656]\n",
      "[ 0.97607017]\n",
      "[ 0.97679442]\n",
      "[ 0.97637802]\n",
      "[ 0.97897774]\n",
      "[ 0.97597468]\n",
      "[ 0.9812212]\n",
      "[ 0.97917777]\n",
      "[ 0.98208404]\n",
      "[ 0.98338997]\n",
      "[ 0.97963035]\n",
      "[ 0.98346329]\n",
      "[ 0.98103648]\n",
      "[ 0.97841984]\n",
      "[ 0.97499555]\n",
      "[ 0.97418714]\n",
      "[ 0.98221713]\n",
      "[ 0.9743185]\n",
      "[ 0.97433197]\n",
      "[ 0.9760586]\n",
      "[ 0.97667581]\n",
      "[ 0.97998434]\n",
      "[ 0.98533607]\n",
      "[ 0.98465157]\n",
      "[ 0.9811939]\n",
      "[ 0.98244935]\n",
      "[ 0.98594749]\n",
      "[ 0.98423308]\n",
      "[ 0.98176754]\n",
      "[ 0.98518097]\n",
      "[ 0.9810096]\n",
      "[ 0.98003352]\n",
      "[ 0.97205293]\n",
      "[ 0.98178715]\n",
      "[ 0.97381067]\n",
      "[ 0.98127681]\n",
      "[ 0.97868556]\n",
      "[ 0.98269111]\n",
      "[ 0.98182654]\n",
      "[ 0.97811508]\n",
      "[ 0.98462087]\n",
      "[ 0.97896338]\n",
      "[ 0.9802745]\n",
      "[ 0.98238748]\n",
      "[ 0.98605508]\n",
      "[ 0.97164959]\n",
      "[ 0.98430723]\n",
      "[ 0.97632456]\n",
      "[ 0.98486036]\n",
      "[ 0.98108321]\n",
      "[ 0.98166537]\n",
      "[ 0.982503]\n",
      "[ 0.97747034]\n",
      "[ 0.97874606]\n",
      "[ 0.9693436]\n",
      "[ 0.97213441]\n",
      "[ 0.97369385]\n",
      "[ 0.97018164]\n",
      "[ 0.97237331]\n",
      "[ 0.97208256]\n",
      "[ 0.96771896]\n",
      "[ 0.96737307]\n",
      "[ 0.9685033]\n",
      "[ 0.97796732]\n",
      "[ 0.97776848]\n",
      "[ 0.97921735]\n",
      "[ 0.9822942]\n",
      "[ 0.98374313]\n",
      "[ 0.98185533]\n",
      "[ 0.98335934]\n",
      "[ 0.98036247]\n",
      "[ 0.98050237]\n",
      "[ 0.98526645]\n",
      "[ 0.98137748]\n",
      "[ 0.98258621]\n",
      "[ 0.98081446]\n",
      "[ 0.98459458]\n",
      "[ 0.984088]\n",
      "[ 0.98545432]\n",
      "[ 0.97845894]\n",
      "[ 0.9836148]\n",
      "[ 0.98103577]\n",
      "[ 0.98324072]\n",
      "[ 0.97977829]\n",
      "[ 0.98039645]\n",
      "[ 0.9811247]\n",
      "[ 0.98337024]\n",
      "[ 0.98220944]\n",
      "[ 0.98242795]\n",
      "[ 0.9835186]\n",
      "[ 0.98265296]\n",
      "[ 0.98122865]\n",
      "[ 0.9823907]\n",
      "[ 0.98294413]\n",
      "[ 0.97962761]\n",
      "[ 0.98052561]\n",
      "[ 0.98363864]\n",
      "[ 0.982777]\n",
      "[ 0.98512435]\n",
      "[ 0.98191482]\n",
      "[ 0.98364615]\n",
      "[ 0.98144352]\n",
      "[ 0.98538131]\n",
      "[ 0.97947508]\n",
      "[ 0.98297101]\n",
      "[ 0.97773165]\n",
      "[ 0.97819144]\n",
      "[ 0.98545271]\n",
      "[ 0.98067981]\n",
      "[ 0.98418456]\n",
      "[ 0.98008579]\n",
      "[ 0.98491436]\n",
      "[ 0.97853899]\n",
      "[ 0.98207313]\n",
      "[ 0.98020041]\n",
      "[ 0.9855504]\n",
      "[ 0.98520672]\n",
      "[ 0.98348898]\n",
      "[ 0.97830462]\n",
      "[ 0.98176968]\n",
      "[ 0.98060673]\n",
      "[ 0.98048842]\n",
      "[ 0.98194861]\n",
      "[ 0.97371995]\n",
      "[ 0.97919029]\n",
      "[ 0.98211944]\n",
      "[ 0.98263502]\n",
      "[ 0.9819259]\n",
      "[ 0.9829942]\n",
      "[ 0.62553376]\n",
      "[ 0.66952175]\n",
      "[ 0.83832002]\n",
      "[ 0.8207497]\n",
      "[ 0.89231229]\n",
      "[ 0.86471444]\n",
      "[ 0.88526928]\n",
      "[ 0.87809449]\n",
      "[ 0.94254088]\n",
      "[ 0.95397902]\n",
      "[ 0.96530116]\n",
      "[ 0.95927346]\n",
      "[ 0.95778626]\n",
      "[ 0.89588732]\n",
      "[ 0.86087525]\n",
      "[ 0.90659422]\n",
      "[ 0.89867133]\n",
      "[ 0.94304806]\n",
      "[ 0.93162388]\n",
      "[ 0.9297601]\n",
      "[ 0.91603279]\n",
      "[ 0.95226634]\n",
      "[ 0.95483488]\n",
      "[ 0.95844102]\n",
      "[ 0.95841271]\n",
      "[ 0.94882655]\n",
      "[ 0.95300776]\n",
      "[ 0.9526118]\n",
      "[ 0.94098252]\n",
      "[ 0.96519643]\n",
      "[ 0.96690261]\n",
      "[ 0.96973813]\n",
      "[ 0.95961398]\n",
      "[ 0.96267539]\n",
      "[ 0.9581269]\n",
      "[ 0.94812983]\n",
      "[ 0.96713704]\n",
      "[ 0.95863241]\n",
      "[ 0.96911329]\n",
      "[ 0.95131505]\n",
      "[ 0.95617819]\n",
      "[ 0.95011514]\n",
      "[ 0.97171468]\n",
      "[ 0.95097119]\n",
      "[ 0.95362014]\n",
      "[ 0.96274596]\n",
      "[ 0.96912283]\n",
      "[ 0.96074307]\n",
      "[ 0.9741953]\n",
      "[ 0.97778708]\n",
      "[ 0.97544664]\n",
      "[ 0.97525054]\n",
      "[ 0.97898275]\n",
      "[ 0.97973901]\n",
      "[ 0.9770059]\n",
      "[ 0.97228855]\n",
      "[ 0.97215289]\n",
      "[ 0.97543842]\n",
      "[ 0.98259223]\n",
      "[ 0.9811365]\n",
      "[ 0.98179227]\n",
      "[ 0.9847663]\n",
      "[ 0.97788733]\n",
      "[ 0.98439711]\n",
      "[ 0.98305708]\n",
      "[ 0.97987336]\n",
      "[ 0.98202276]\n",
      "[ 0.98068595]\n",
      "[ 0.9819743]\n",
      "[ 0.98111582]\n",
      "[ 0.98568422]\n",
      "[ 0.98454148]\n",
      "[ 0.97840303]\n",
      "[ 0.98345459]\n",
      "[ 0.97905481]\n",
      "[ 0.97938275]\n",
      "[ 0.97760808]\n",
      "[ 0.97981954]\n",
      "[ 0.97732258]\n",
      "[ 0.96973604]\n",
      "[ 0.9816584]\n",
      "[ 0.98534673]\n",
      "[ 0.98140252]\n",
      "[ 0.98124552]\n",
      "[ 0.97633731]\n",
      "[ 0.97603363]\n",
      "[ 0.97481889]\n",
      "[ 0.97756767]\n",
      "[ 0.97826713]\n",
      "[ 0.9813444]\n",
      "[ 0.98121113]\n",
      "[ 0.98144209]\n",
      "[ 0.97751492]\n",
      "[ 0.98578221]\n",
      "[ 0.98416948]\n",
      "[ 0.98148286]\n",
      "[ 0.98492253]\n",
      "[ 0.97949916]\n",
      "[ 0.97394824]\n",
      "[ 0.98211789]\n",
      "[ 0.9797172]\n",
      "[ 0.9696272]\n",
      "[ 0.97460675]\n",
      "[ 0.97669476]\n",
      "[ 0.97372961]\n",
      "[ 0.97685707]\n",
      "[ 0.97304702]\n",
      "[ 0.9763388]\n",
      "[ 0.96029431]\n",
      "[ 0.96190214]\n",
      "[ 0.95993084]\n",
      "[ 0.96077871]\n",
      "[ 0.96958649]\n",
      "[ 0.97744679]\n",
      "[ 0.97373408]\n",
      "[ 0.97415841]\n",
      "[ 0.97646081]\n",
      "[ 0.97227877]\n",
      "[ 0.97360879]\n",
      "[ 0.97746748]\n",
      "[ 0.96112543]\n",
      "[ 0.96623403]\n",
      "[ 0.92588884]\n",
      "[ 0.94538242]\n",
      "[ 0.95349479]\n",
      "[ 0.95137006]\n",
      "[ 0.95615488]\n",
      "[ 0.95728815]\n",
      "[ 0.95860785]\n",
      "[ 0.96763211]\n",
      "[ 0.95797753]\n",
      "[ 0.94099432]\n",
      "[ 0.96345121]\n",
      "[ 0.95933622]\n",
      "[ 0.96338135]\n",
      "[ 0.97444963]\n",
      "[ 0.96389586]\n",
      "[ 0.96670479]\n",
      "[ 0.96541739]\n",
      "[ 0.97143292]\n",
      "[ 0.9690327]\n",
      "[ 0.97915345]\n",
      "[ 0.98176199]\n",
      "[ 0.97563988]\n",
      "[ 0.97680545]\n",
      "[ 0.97176844]\n",
      "[ 0.9757663]\n",
      "[ 0.97084498]\n",
      "[ 0.97290486]\n",
      "[ 0.97254568]\n",
      "[ 0.96411246]\n",
      "[ 0.95414799]\n",
      "[ 0.9619208]\n",
      "[ 0.96251124]\n",
      "[ 0.9600873]\n",
      "[ 0.96759993]\n",
      "[ 0.96599877]\n",
      "[ 0.95570934]\n",
      "[ 0.96650761]\n",
      "[ 0.97208363]\n",
      "[ 0.97074372]\n",
      "[ 0.96845007]\n",
      "[ 0.96217555]\n",
      "[ 0.97232527]\n",
      "[ 0.97831964]\n",
      "[ 0.97383261]\n",
      "[ 0.97287649]\n",
      "[ 0.9771145]\n",
      "[ 0.98623538]\n",
      "[ 0.98201859]\n",
      "[ 0.98182291]\n",
      "[ 0.97998941]\n",
      "[ 0.97809273]\n",
      "[ 0.97806114]\n",
      "[ 0.97932798]\n",
      "[ 0.98180115]\n",
      "[ 0.98060775]\n",
      "[ 0.98194349]\n",
      "[ 0.98178047]\n",
      "[ 0.98179436]\n",
      "[ 0.98225152]\n",
      "[ 0.98088974]\n",
      "[ 0.98327172]\n",
      "[ 0.97933906]\n",
      "[ 0.98320711]\n",
      "[ 0.9826591]\n",
      "[ 0.98446411]\n",
      "[ 0.98021668]\n",
      "[ 0.98060817]\n",
      "[ 0.98223799]\n",
      "[ 0.98143905]\n",
      "[ 0.9833411]\n",
      "[ 0.98113358]\n",
      "[ 0.97948563]\n",
      "[ 0.98415399]\n",
      "[ 0.97520691]\n",
      "[ 0.98525143]\n",
      "[ 0.98098648]\n",
      "[ 0.98513252]\n",
      "[ 0.98118573]\n",
      "[ 0.59642309]\n",
      "[ 0.5768013]\n",
      "[ 0.63205427]\n",
      "[ 0.65356296]\n",
      "[ 0.61758256]\n",
      "[ 0.58422929]\n",
      "[ 0.53180867]\n",
      "[ 0.56928807]\n",
      "[ 0.58230138]\n",
      "[ 0.58482426]\n",
      "[ 0.59597057]\n",
      "[ 0.6070019]\n",
      "[ 0.59007329]\n",
      "[ 0.5847041]\n",
      "[ 0.58815408]\n",
      "[ 0.56813091]\n",
      "[ 0.60292786]\n",
      "[ 0.51228553]\n",
      "[ 0.53125334]\n",
      "[ 0.54110217]\n",
      "[ 0.49199706]\n",
      "[ 0.5245142]\n",
      "[ 0.5523771]\n",
      "[ 0.63064122]\n",
      "[ 0.64838445]\n",
      "[ 0.60923314]\n",
      "[ 0.44349268]\n",
      "[ 0.38010907]\n",
      "[ 0.35537204]\n",
      "[ 0.3969343]\n",
      "[ 0.37595952]\n",
      "[ 0.41120106]\n",
      "[ 0.62566751]\n",
      "[ 0.68483299]\n",
      "[ 0.64236093]\n",
      "[ 0.58172172]\n",
      "[ 0.62136382]\n",
      "[ 0.62920982]\n",
      "[ 0.60115045]\n",
      "[ 0.59620404]\n",
      "[ 0.64559937]\n",
      "[ 0.57708406]\n",
      "[ 0.61549985]\n",
      "[ 0.59484637]\n",
      "[ 0.58291084]\n",
      "[ 0.55770278]\n",
      "[ 0.56966192]\n",
      "[ 0.52649182]\n",
      "[ 0.54289979]\n",
      "[ 0.57050574]\n",
      "[ 0.59099728]\n",
      "[ 0.5760681]\n",
      "[ 0.55085647]\n",
      "[ 0.57594514]\n",
      "[ 0.6304698]\n",
      "[ 0.6076197]\n",
      "[ 0.51006681]\n",
      "[ 0.56711394]\n",
      "[ 0.48529223]\n",
      "[ 0.52709055]\n",
      "[ 0.53556317]\n",
      "[ 0.51999313]\n",
      "[ 0.76214194]\n",
      "[ 0.73650527]\n",
      "[ 0.79999602]\n",
      "[ 0.73690784]\n",
      "[ 0.77149284]\n",
      "[ 0.73410463]\n",
      "[ 0.77851737]\n",
      "[ 0.73778677]\n",
      "[ 0.76639777]\n",
      "[ 0.74707067]\n",
      "[ 0.77013934]\n",
      "[ 0.72161061]\n",
      "[ 0.71020383]\n",
      "[ 0.7061215]\n",
      "[ 0.7069025]\n",
      "[ 0.88728249]\n",
      "[ 0.85930228]\n",
      "[ 0.87714893]\n",
      "[ 0.86819106]\n",
      "[ 0.82399905]\n",
      "[ 0.848957]\n",
      "[ 0.79381198]\n",
      "[ 0.85169202]\n",
      "[ 0.85352427]\n",
      "[ 0.7920655]\n",
      "[ 0.70913476]\n",
      "[ 0.72338367]\n",
      "[ 0.71288025]\n",
      "[ 0.70208246]\n",
      "[ 0.70514554]\n",
      "[ 0.66077274]\n",
      "[ 0.64599001]\n",
      "[ 0.73589885]\n",
      "[ 0.68763101]\n",
      "[ 0.71706074]\n",
      "[ 0.72509581]\n",
      "[ 0.64458168]\n",
      "[ 0.69207591]\n",
      "[ 0.71054071]\n",
      "[ 0.73569435]\n",
      "[ 0.82919371]\n",
      "[ 0.81600046]\n",
      "[ 0.86094642]\n",
      "[ 0.87283432]\n",
      "[ 0.87083489]\n",
      "[ 0.89289182]\n",
      "[ 0.9103142]\n",
      "[ 0.91622221]\n",
      "[ 0.88913184]\n",
      "[ 0.89504164]\n",
      "[ 0.88657808]\n",
      "[ 0.869362]\n",
      "[ 0.8602851]\n",
      "[ 0.86949581]\n",
      "[ 0.87747222]\n",
      "[ 0.85238701]\n",
      "[ 0.74553359]\n",
      "[ 0.81027937]\n",
      "[ 0.82250994]\n",
      "[ 0.84449333]\n",
      "[ 0.93235815]\n",
      "[ 0.90946269]\n",
      "[ 0.90993226]\n",
      "[ 0.91797781]\n",
      "[ 0.9115842]\n",
      "[ 0.89162385]\n",
      "[ 0.904818]\n",
      "[ 0.8792634]\n",
      "[ 0.86919916]\n",
      "[ 0.86912322]\n",
      "[ 0.88814181]\n",
      "[ 0.90046364]\n",
      "[ 0.91663206]\n",
      "[ 0.92404747]\n",
      "[ 0.926974]\n",
      "[ 0.88261729]\n",
      "[ 0.88402277]\n",
      "[ 0.87983656]\n",
      "[ 0.82616329]\n",
      "[ 0.8425709]\n",
      "[ 0.86081231]\n",
      "[ 0.83845854]\n",
      "[ 0.8494966]\n",
      "[ 0.83822501]\n",
      "[ 0.94227618]\n",
      "[ 0.92834979]\n",
      "[ 0.94229388]\n",
      "[ 0.91843981]\n",
      "[ 0.97620738]\n",
      "[ 0.98350292]\n",
      "[ 0.98570049]\n",
      "[ 0.98397893]\n",
      "[ 0.98228282]\n",
      "[ 0.98061073]\n",
      "[ 0.97672594]\n",
      "[ 0.97615933]\n",
      "[ 0.97995007]\n",
      "[ 0.98214573]\n",
      "[ 0.97890794]\n",
      "[ 0.97739547]\n",
      "[ 0.97690606]\n",
      "[ 0.9756003]\n",
      "[ 0.97930658]\n",
      "[ 0.97574407]\n",
      "[ 0.97481811]\n",
      "[ 0.97932857]\n",
      "[ 0.97657359]\n",
      "[ 0.98132777]\n",
      "[ 0.98335493]\n",
      "[ 0.98216015]\n",
      "[ 0.97929597]\n",
      "[ 0.97995573]\n",
      "[ 0.97274899]\n",
      "[ 0.97908348]\n",
      "[ 0.97751194]\n",
      "[ 0.98396164]\n",
      "[ 0.98140341]\n",
      "[ 0.9813965]\n",
      "[ 0.98342186]\n",
      "[ 0.98311967]\n",
      "[ 0.98229063]\n",
      "[ 0.97800785]\n",
      "[ 0.98512799]\n",
      "[ 0.97991318]\n",
      "[ 0.98480874]\n",
      "[ 0.98343927]\n",
      "[ 0.98011702]\n",
      "[ 0.98224044]\n",
      "[ 0.98367459]\n",
      "[ 0.97778547]\n",
      "[ 0.97590798]\n",
      "[ 0.97969908]\n",
      "[ 0.97884172]\n",
      "[ 0.97701687]\n",
      "[ 0.97620589]\n",
      "[ 0.97532737]\n",
      "[ 0.97872514]\n",
      "[ 0.97639173]\n",
      "[ 0.49447533]\n",
      "[ 0.4967725]\n",
      "[ 0.49767217]\n",
      "[ 0.49793971]\n",
      "[ 0.49800527]\n",
      "[ 0.49801993]\n",
      "[ 0.49802265]\n",
      "[ 0.49802276]\n",
      "[ 0.49802247]\n",
      "[ 0.49802229]\n",
      "[ 0.49802229]\n",
      "[ 0.49802241]\n",
      "[ 0.49802256]\n",
      "[ 0.49802276]\n",
      "[ 0.49802294]\n",
      "[ 0.49802309]\n",
      "[ 0.49802321]\n",
      "[ 0.49802336]\n",
      "[ 0.49802345]\n",
      "[ 0.49802351]\n",
      "[ 0.49802357]\n",
      "[ 0.49802363]\n",
      "[ 0.49802366]\n",
      "[ 0.49802369]\n",
      "[ 0.49802372]\n",
      "[ 0.49802375]\n",
      "[ 0.49802375]\n",
      "[ 0.49802375]\n",
      "[ 0.49802375]\n",
      "[ 0.49802378]\n",
      "[ 0.49802378]\n",
      "[ 0.49802378]\n",
      "[ 0.49802378]\n",
      "[ 0.49802378]\n",
      "[ 0.49802378]\n",
      "[ 0.49802378]\n",
      "[ 0.49802378]\n",
      "[ 0.49802378]\n",
      "[ 0.49802378]\n",
      "[ 0.49802378]\n",
      "[ 0.49802381]\n",
      "[ 0.49802378]\n",
      "[ 0.49802381]\n",
      "[ 0.49802378]\n",
      "[ 0.49802381]\n",
      "[ 0.49802378]\n",
      "[ 0.49802381]\n",
      "[ 0.49802378]\n",
      "[ 0.49802381]\n",
      "[ 0.49802378]\n",
      "[ 0.49802381]\n",
      "[ 0.49802378]\n",
      "[ 0.49802381]\n",
      "[ 0.49802378]\n",
      "[ 0.49802381]\n",
      "[ 0.49802378]\n",
      "[ 0.49802381]\n",
      "[ 0.49802378]\n",
      "[ 0.49802381]\n",
      "[ 0.49802378]\n",
      "[ 0.49802381]\n",
      "[ 0.49802378]\n",
      "[ 0.49802381]\n",
      "[ 0.49802378]\n",
      "[ 0.47142845]\n",
      "[ 0.31548718]\n",
      "[ 0.31361696]\n",
      "[ 0.26839036]\n",
      "[ 0.21405728]\n",
      "[ 0.14723457]\n",
      "[ 0.14048485]\n",
      "[ 0.16952282]\n",
      "[ 0.06966686]\n",
      "[ 0.06098653]\n",
      "[ 0.04343582]\n",
      "[ 0.04972004]\n",
      "[ 0.06085068]\n",
      "[ 0.05033661]\n",
      "[ 0.03985739]\n",
      "[ 0.04225771]\n",
      "[ 0.04473111]\n",
      "[ 0.0304953]\n",
      "[ 0.03348532]\n",
      "[ 0.03390687]\n",
      "[ 0.03500229]\n",
      "[ 0.03873533]\n",
      "[ 0.03967965]\n",
      "[ 0.04215861]\n",
      "[ 0.04542807]\n",
      "[ 0.02952859]\n",
      "[ 0.0374366]\n",
      "[ 0.03202423]\n",
      "[ 0.03774239]\n",
      "[ 0.0441668]\n",
      "[ 0.04527622]\n",
      "[ 0.03758559]\n",
      "[ 0.05209624]\n",
      "[ 0.05532886]\n",
      "[ 0.0581915]\n",
      "[ 0.07546933]\n",
      "[ 0.06218369]\n",
      "[ 0.07088124]\n",
      "[ 0.08562241]\n",
      "[ 0.08695381]\n",
      "[ 0.1077186]\n",
      "[ 0.10814567]\n",
      "[ 0.1119361]\n",
      "[ 0.14090931]\n",
      "[ 0.14733736]\n",
      "[ 0.10920356]\n",
      "[ 0.23858701]\n",
      "[ 0.46845108]\n",
      "[ 0.43096304]\n",
      "[ 0.44382459]\n",
      "[ 0.48811257]\n",
      "[ 0.50142968]\n",
      "[ 0.51787877]\n",
      "[ 0.47714365]\n",
      "[ 0.34253308]\n",
      "[ 0.35575616]\n",
      "[ 0.34725955]\n",
      "[ 0.34192228]\n",
      "[ 0.36375433]\n",
      "[ 0.38754568]\n",
      "[ 0.47407997]\n",
      "[ 0.40596798]\n",
      "[ 0.37705898]\n",
      "[ 0.39400566]\n",
      "[ 0.35985723]\n",
      "[ 0.39358792]\n",
      "[ 0.4304097]\n",
      "[ 0.40555665]\n",
      "[ 0.41134334]\n",
      "[ 0.27040142]\n",
      "[ 0.30380857]\n",
      "[ 0.16575161]\n",
      "[ 0.16461185]\n",
      "[ 0.16317342]\n",
      "[ 0.19219333]\n",
      "[ 0.22963838]\n",
      "[ 0.26665881]\n",
      "[ 0.25653297]\n",
      "[ 0.25886646]\n",
      "[ 0.26206121]\n",
      "[ 0.28873882]\n",
      "[ 0.25834233]\n",
      "[ 0.29681468]\n",
      "[ 0.29427576]\n",
      "[ 0.26987052]\n",
      "[ 0.25396994]\n",
      "[ 0.23262933]\n",
      "[ 0.2600894]\n",
      "[ 0.24231336]\n",
      "[ 0.30430833]\n",
      "[ 0.29768372]\n",
      "[ 0.2385807]\n",
      "[ 0.34410667]\n",
      "[ 0.27134645]\n",
      "[ 0.35559696]\n",
      "[ 0.39185703]\n",
      "[ 0.41904706]\n",
      "[ 0.42562935]\n",
      "[ 0.38201311]\n",
      "[ 0.37078846]\n",
      "[ 0.39522922]\n",
      "[ 0.44243833]\n",
      "[ 0.50193572]\n",
      "[ 0.3451162]\n",
      "[ 0.33706096]\n",
      "[ 0.35397235]\n",
      "[ 0.35158479]\n",
      "[ 0.35526821]\n",
      "[ 0.40719098]\n",
      "[ 0.33235252]\n",
      "[ 0.3592]\n",
      "[ 0.33454284]\n",
      "[ 0.41994661]\n",
      "[ 0.54432476]\n",
      "[ 0.37545359]\n",
      "[ 0.4489468]\n",
      "[ 0.46786872]\n",
      "[ 0.46861795]\n",
      "[ 0.26435128]\n",
      "[ 0.33605763]\n",
      "[ 0.1882617]\n",
      "[ 0.17315225]\n",
      "[ 0.16526726]\n",
      "[ 0.22007236]\n",
      "[ 0.12148033]\n",
      "[ 0.15489434]\n",
      "[ 0.12842357]\n",
      "[ 0.14867619]\n",
      "[ 0.14230199]\n",
      "[ 0.18835692]\n",
      "[ 0.16564864]\n",
      "[ 0.21346729]\n",
      "[ 0.17342712]\n",
      "[ 0.27069637]\n",
      "[ 0.3248871]\n",
      "[ 0.38725296]\n",
      "[ 0.49447533]\n",
      "[ 0.4967725]\n",
      "[ 0.49767217]\n",
      "[ 0.49793971]\n",
      "[ 0.49800527]\n",
      "[ 0.49801993]\n",
      "[ 0.49802265]\n",
      "[ 0.49802276]\n",
      "[ 0.49802247]\n",
      "[ 0.49802229]\n",
      "[ 0.49802229]\n",
      "[ 0.49802241]\n",
      "[ 0.49802256]\n",
      "[ 0.49802276]\n",
      "[ 0.49802294]\n",
      "[ 0.49802309]\n",
      "[ 0.49802321]\n",
      "[ 0.49802336]\n",
      "[ 0.49802345]\n",
      "[ 0.49802351]\n",
      "[ 0.49802357]\n",
      "[ 0.49802363]\n",
      "[ 0.49802366]\n",
      "[ 0.49802369]\n",
      "[ 0.49802372]\n",
      "[ 0.49802375]\n",
      "[ 0.49802375]\n",
      "[ 0.49802375]\n",
      "[ 0.49802375]\n",
      "[ 0.49802378]\n",
      "[ 0.49802378]\n",
      "[ 0.49802378]\n",
      "[ 0.49802378]\n",
      "[ 0.49802378]\n",
      "[ 0.49802378]\n",
      "[ 0.46898928]\n",
      "[ 0.47150561]\n",
      "[ 0.51753801]\n",
      "[ 0.89194959]\n",
      "[ 0.91682297]\n",
      "[ 0.92998403]\n",
      "[ 0.9322806]\n",
      "[ 0.93919182]\n",
      "[ 0.93789119]\n",
      "[ 0.94589084]\n",
      "[ 0.95191205]\n",
      "[ 0.95896471]\n",
      "[ 0.96913975]\n",
      "[ 0.96629363]\n",
      "[ 0.97001731]\n",
      "[ 0.97458172]\n",
      "[ 0.97549188]\n",
      "[ 0.97905171]\n",
      "[ 0.9763127]\n",
      "[ 0.97629017]\n",
      "[ 0.97843254]\n",
      "[ 0.97938925]\n",
      "[ 0.97355217]\n",
      "[ 0.97795111]\n",
      "[ 0.96967828]\n",
      "[ 0.97937775]\n",
      "[ 0.98145443]\n",
      "[ 0.97303301]\n",
      "[ 0.98343366]\n",
      "[ 0.97802728]\n",
      "[ 0.98360932]\n",
      "[ 0.98401123]\n",
      "[ 0.97850275]\n",
      "[ 0.98271316]\n",
      "[ 0.97967315]\n",
      "[ 0.97813886]\n",
      "[ 0.98189771]\n",
      "[ 0.98313671]\n",
      "[ 0.98153174]\n",
      "[ 0.9783904]\n",
      "[ 0.97321761]\n",
      "[ 0.98025393]\n",
      "[ 0.9788003]\n",
      "[ 0.98138112]\n",
      "[ 0.97826672]\n",
      "[ 0.97710723]\n",
      "[ 0.9835493]\n",
      "[ 0.98146749]\n",
      "[ 0.98271602]\n",
      "[ 0.9838962]\n",
      "[ 0.97956866]\n",
      "[ 0.98361558]\n",
      "[ 0.97815979]\n",
      "[ 0.98298544]\n",
      "[ 0.97967321]\n",
      "[ 0.98061514]\n",
      "[ 0.98194045]\n",
      "[ 0.97615731]\n",
      "[ 0.98329473]\n",
      "[ 0.98460132]\n",
      "[ 0.98089463]\n",
      "[ 0.98417217]\n",
      "[ 0.9842481]\n",
      "[ 0.9833622]\n",
      "[ 0.98298997]\n",
      "[ 0.98383445]\n",
      "[ 0.97889596]\n",
      "[ 0.97997862]\n",
      "[ 0.96655083]\n",
      "[ 0.97873229]\n",
      "[ 0.97789288]\n",
      "[ 0.98197544]\n",
      "[ 0.97952503]\n",
      "[ 0.9848969]\n",
      "[ 0.98291856]\n",
      "[ 0.98304904]\n",
      "[ 0.97556937]\n",
      "[ 0.98231387]\n",
      "[ 0.97367054]\n",
      "[ 0.9760052]\n",
      "[ 0.97600377]\n",
      "[ 0.98358554]\n",
      "[ 0.97952831]\n",
      "[ 0.98025519]\n",
      "[ 0.97390032]\n",
      "[ 0.98211819]\n",
      "[ 0.98189151]\n",
      "[ 0.97811055]\n",
      "[ 0.98394793]\n",
      "[ 0.96637326]\n",
      "[ 0.97225881]\n",
      "[ 0.97597128]\n",
      "[ 0.97783351]\n",
      "[ 0.97389841]\n",
      "[ 0.97521031]\n",
      "[ 0.98118865]\n",
      "[ 0.97934103]\n",
      "[ 0.98235935]\n",
      "[ 0.98296171]\n",
      "[ 0.9829008]\n",
      "[ 0.98103809]\n",
      "[ 0.98361045]\n",
      "[ 0.98420328]\n",
      "[ 0.98178595]\n",
      "[ 0.96846116]\n",
      "[ 0.97483695]\n",
      "[ 0.97147995]\n",
      "[ 0.97963679]\n",
      "[ 0.98153639]\n",
      "[ 0.98088062]\n",
      "[ 0.98201561]\n",
      "[ 0.98289257]\n",
      "[ 0.98245728]\n",
      "[ 0.98382533]\n",
      "[ 0.98488104]\n",
      "[ 0.98483855]\n",
      "[ 0.98243475]\n",
      "[ 0.98095071]\n",
      "[ 0.98062408]\n",
      "[ 0.98367172]\n",
      "[ 0.98220044]\n",
      "[ 0.97848845]\n",
      "[ 0.97845262]\n",
      "[ 0.98065782]\n",
      "[ 0.97852933]\n",
      "[ 0.97968239]\n",
      "[ 0.97584176]\n",
      "[ 0.9817462]\n",
      "[ 0.98206419]\n",
      "[ 0.97578269]\n",
      "[ 0.97861642]\n",
      "[ 0.97762614]\n",
      "[ 0.97830808]\n",
      "[ 0.97859192]\n",
      "[ 0.97491795]\n",
      "[ 0.97874278]\n",
      "[ 0.97678399]\n",
      "[ 0.9673084]\n",
      "[ 0.96887422]\n",
      "[ 0.97160101]\n",
      "[ 0.96761686]\n",
      "[ 0.9690665]\n",
      "[ 0.97991061]\n",
      "[ 0.97570682]\n",
      "[ 0.97209674]\n",
      "[ 0.97412759]\n",
      "[ 0.97619385]\n",
      "[ 0.97777838]\n",
      "[ 0.98007596]\n",
      "[ 0.97520036]\n",
      "[ 0.97405869]\n",
      "[ 0.97663289]\n",
      "[ 0.97757643]\n",
      "[ 0.98102313]\n",
      "[ 0.98249769]\n",
      "[ 0.98564279]\n",
      "[ 0.98061079]\n",
      "[ 0.98368037]\n",
      "[ 0.98332781]\n",
      "[ 0.97563154]\n",
      "[ 0.97953302]\n",
      "[ 0.97187769]\n",
      "[ 0.96444458]\n",
      "[ 0.96917123]\n",
      "[ 0.97671711]\n",
      "[ 0.48219889]\n",
      "[ 0.50216764]\n",
      "[ 0.55852711]\n",
      "[ 0.67686349]\n",
      "[ 0.69984591]\n",
      "[ 0.63583231]\n",
      "[ 0.68040943]\n",
      "[ 0.7022959]\n",
      "[ 0.65477306]\n",
      "[ 0.6503455]\n",
      "[ 0.63331455]\n",
      "[ 0.50541419]\n",
      "[ 0.50676095]\n",
      "[ 0.49000221]\n",
      "[ 0.48697051]\n",
      "[ 0.4525331]\n",
      "[ 0.46389493]\n",
      "[ 0.49135849]\n",
      "[ 0.53859645]\n",
      "[ 0.50529939]\n",
      "[ 0.52874088]\n",
      "[ 0.55564129]\n",
      "[ 0.57329303]\n",
      "[ 0.55250961]\n",
      "[ 0.53665829]\n",
      "[ 0.53696048]\n",
      "[ 0.57617068]\n",
      "[ 0.58254033]\n",
      "[ 0.44452828]\n",
      "[ 0.46096969]\n",
      "[ 0.51062584]\n",
      "[ 0.58552039]\n",
      "[ 0.56211877]\n",
      "[ 0.58746541]\n",
      "[ 0.51666409]\n",
      "[ 0.56674975]\n",
      "[ 0.55336022]\n",
      "[ 0.56911194]\n",
      "[ 0.58393216]\n",
      "[ 0.55462885]\n",
      "[ 0.56329149]\n",
      "[ 0.50773841]\n",
      "[ 0.52907872]\n",
      "[ 0.49897069]\n",
      "[ 0.56092113]\n",
      "[ 0.54320848]\n",
      "[ 0.53860646]\n",
      "[ 0.59433919]\n",
      "[ 0.63997614]\n",
      "[ 0.59212041]\n",
      "[ 0.62628913]\n",
      "[ 0.61703664]\n",
      "[ 0.60098058]\n",
      "[ 0.65827626]\n",
      "[ 0.74893397]\n",
      "[ 0.68398458]\n",
      "[ 0.75552499]\n",
      "[ 0.70246601]\n",
      "[ 0.71805072]\n",
      "[ 0.76762879]\n",
      "[ 0.69650882]\n",
      "[ 0.70951229]\n",
      "[ 0.68968999]\n",
      "[ 0.62145561]\n",
      "[ 0.59493315]\n",
      "[ 0.58411783]\n",
      "[ 0.56328756]\n",
      "[ 0.55699986]\n",
      "[ 0.49387345]\n",
      "[ 0.59358472]\n",
      "[ 0.56021726]\n",
      "[ 0.51518583]\n",
      "[ 0.51420408]\n",
      "[ 0.5178799]\n",
      "[ 0.47807825]\n",
      "[ 0.49739346]\n",
      "[ 0.55445951]\n",
      "[ 0.5653038]\n",
      "[ 0.60019761]\n",
      "[ 0.56943661]\n",
      "[ 0.58594549]\n",
      "[ 0.59850472]\n",
      "[ 0.59941572]\n",
      "[ 0.59415883]\n",
      "[ 0.59247822]\n",
      "[ 0.61044252]\n",
      "[ 0.56631774]\n",
      "[ 0.59405905]\n",
      "[ 0.52870864]\n",
      "[ 0.51779044]\n",
      "[ 0.53403842]\n",
      "[ 0.53258765]\n",
      "[ 0.49958307]\n",
      "[ 0.54789156]\n",
      "[ 0.50176978]\n",
      "[ 0.51392907]\n",
      "[ 0.58081526]\n",
      "[ 0.5782789]\n",
      "[ 0.53221208]\n",
      "[ 0.53341901]\n",
      "[ 0.50273484]\n",
      "[ 0.58555657]\n",
      "[ 0.64082575]\n",
      "[ 0.73364162]\n",
      "[ 0.75857681]\n",
      "[ 0.75439626]\n",
      "[ 0.71284634]\n",
      "[ 0.74764556]\n",
      "[ 0.70504135]\n",
      "[ 0.79185259]\n",
      "[ 0.81571358]\n",
      "[ 0.8072648]\n",
      "[ 0.81285393]\n",
      "[ 0.83044434]\n",
      "[ 0.81761354]\n",
      "[ 0.82783192]\n",
      "[ 0.84558076]\n",
      "[ 0.81664997]\n",
      "[ 0.80779463]\n",
      "[ 0.79414308]\n",
      "[ 0.77228749]\n",
      "[ 0.81726921]\n",
      "[ 0.78206986]\n",
      "[ 0.81295866]\n",
      "[ 0.81548882]\n",
      "[ 0.79291409]\n",
      "[ 0.82041639]\n",
      "[ 0.8596788]\n",
      "[ 0.81555986]\n",
      "[ 0.84639144]\n",
      "[ 0.85126644]\n",
      "[ 0.90628183]\n",
      "[ 0.89366627]\n",
      "[ 0.91312546]\n",
      "[ 0.83917272]\n",
      "[ 0.89808375]\n",
      "[ 0.87608147]\n",
      "[ 0.85750264]\n",
      "[ 0.94216549]\n",
      "[ 0.93759638]\n",
      "[ 0.94479525]\n",
      "[ 0.96345687]\n",
      "[ 0.97108907]\n",
      "[ 0.96481621]\n",
      "[ 0.96964836]\n",
      "[ 0.9735623]\n",
      "[ 0.95092613]\n",
      "[ 0.94977194]\n",
      "[ 0.94411814]\n",
      "[ 0.94730276]\n",
      "[ 0.95227009]\n",
      "[ 0.95671153]\n",
      "[ 0.97367811]\n",
      "[ 0.96652484]\n",
      "[ 0.97288996]\n",
      "[ 0.96716297]\n",
      "[ 0.95780802]\n",
      "[ 0.95299137]\n",
      "[ 0.95402515]\n",
      "[ 0.95805913]\n",
      "[ 0.95919251]\n",
      "[ 0.97147894]\n",
      "[ 0.97435135]\n",
      "[ 0.96195692]\n",
      "[ 0.96090096]\n",
      "[ 0.96971995]\n",
      "[ 0.97031617]\n",
      "[ 0.97814143]\n",
      "[ 0.97042483]\n",
      "[ 0.9721688]\n",
      "[ 0.97650206]\n",
      "[ 0.97569996]\n",
      "[ 0.977907]\n",
      "[ 0.98010433]\n",
      "[ 0.97693449]\n",
      "[ 0.97104514]\n",
      "[ 0.97029096]\n",
      "[ 0.97323489]\n",
      "[ 0.97327298]\n",
      "[ 0.97502899]\n",
      "[ 0.97594547]\n",
      "[ 0.97308546]\n",
      "[ 0.9790988]\n",
      "[ 0.97469854]\n",
      "[ 0.98255104]\n",
      "[ 0.97900313]\n",
      "[ 0.97835255]\n",
      "[ 0.97700441]\n",
      "[ 0.97872621]\n",
      "[ 0.97530389]\n",
      "[ 0.97589195]\n",
      "[ 0.97589439]\n",
      "[ 0.97744846]\n",
      "[ 0.97693008]\n",
      "[ 0.98193455]\n",
      "[ 0.97979623]\n",
      "[ 0.98207891]\n",
      "[ 0.97727334]\n",
      "[ 0.96997184]\n",
      "[ 0.97910547]\n"
     ]
    }
   ],
   "source": [
    "for review in y_test_pred_mult[:,:,:][:10]:\n",
    "    for word in review:\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 199, 1)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predDelta = y_test_pred_mult[:,1:,:] - y_test_pred_mult[:,:-1,:]\n",
    "predDelta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 200)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# concatenate 0.5 (neutral sentiment) to the initial dimension \n",
    "predDelta = np.concatenate((np.repeat(0.5, 25000).reshape((-1,1)), predDelta.reshape((-1, max_len-1))), axis=1)\n",
    "predDelta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/numpy/core/_methods.py:59: RuntimeWarning: Mean of empty slice.\n",
      "  warnings.warn(\"Mean of empty slice.\", RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "# group the predDelta value by the sequence index value in X_test \n",
    "# to figure out which words cause sentiment to change the most\n",
    "\n",
    "ascDeltaWords = [np.mean(predDelta[X_test == x]) for x in range(max_features)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# filter out nan for words not observed in test set\n",
    "ascDeltaWords = [0 if np.isnan(x) else x for x in ascDeltaWords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Larget positive deltas\n",
      "(444, 'loved', 0.11263533541086045)\n",
      "(477, 'amazing', 0.086340489690133596)\n",
      "(774, 'fantastic', 0.08602834301066184)\n",
      "(318, 'excellent', 0.082009841355638144)\n",
      "(894, 'superb', 0.079432992545851303)\n",
      "(988, 'masterpiece', 0.077322689259000502)\n",
      "(527, 'brilliant', 0.076180880418061495)\n",
      "(511, 'favorite', 0.075837965743177582)\n",
      "(767, 'surprised', 0.071430220788148174)\n",
      "(386, 'wonderful', 0.069415051070398387)\n",
      "(639, 'hilarious', 0.069273067468763175)\n",
      "(250, 'fun', 0.065704363059359519)\n",
      "(401, 'perfect', 0.06416651006781636)\n",
      "(818, 'realistic', 0.061974997869036659)\n",
      "(973, 'powerful', 0.061326236636862493)\n",
      "(773, 'easy', 0.059296769294719977)\n",
      "(438, 'entertaining', 0.058491958900598572)\n",
      "(669, 'episodes', 0.05737955895907218)\n",
      "(830, 'greatest', 0.056969560045111285)\n",
      "(636, 'today', 0.05657306877295256)\n",
      "(507, 'enjoyed', 0.056092979358824888)\n"
     ]
    }
   ],
   "source": [
    "print(\"Larget positive deltas\")\n",
    "count = 0\n",
    "for value in np.argsort(ascDeltaWords).tolist()[::-1]:\n",
    "    #filter to only look at commonly used words\n",
    "    if value < 1000:\n",
    "        count += 1\n",
    "        print((value, intToWord[value], np.mean(predDelta[X_test == value])))\n",
    "        if count > 20:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Larget negative deltas\n",
      "(246, 'worst', -0.15181023224250101)\n",
      "(859, 'poorly', -0.11344948297823375)\n",
      "(370, 'awful', -0.11194030306248676)\n",
      "(469, 'unfortunately', -0.099090252417024224)\n",
      "(750, 'dull', -0.098949559133142129)\n",
      "(524, 'horrible', -0.094292561660854771)\n",
      "(391, 'terrible', -0.090964648518711333)\n",
      "(832, 'lame', -0.086268765115004281)\n",
      "(335, 'poor', -0.083846452531486287)\n",
      "(944, 'mess', -0.083299260013686718)\n",
      "(354, 'boring', -0.077412345219938561)\n",
      "(860, 'premise', -0.076007623730786983)\n",
      "(644, 'ridiculous', -0.074192950603612848)\n",
      "(446, 'oh', -0.072583564464786934)\n",
      "(421, 'supposed', -0.072385803889965766)\n",
      "(682, 'disappointed', -0.069381477845107911)\n",
      "(812, 'weak', -0.068731630262044541)\n",
      "(376, 'stupid', -0.06865531570187737)\n",
      "(434, 'waste', -0.068549884523575508)\n",
      "(861, 'okay', -0.068178222572027852)\n",
      "(993, 'fails', -0.068132868999142962)\n"
     ]
    }
   ],
   "source": [
    "print(\"Larget negative deltas\")\n",
    "count = 0\n",
    "for value in np.argsort(ascDeltaWords).tolist():\n",
    "    #filter to only look at commonly used words\n",
    "    if value < 1000:\n",
    "        count += 1\n",
    "        print((value, intToWord[value], np.mean(predDelta[X_test == value])))\n",
    "        if count > 20:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's pretty cool that with only 25,000 training examples the network was able to learn so accurately what types of words cause a review to transition from positive to negative sentiment or vice versa. I have to image this would only improve with more data. \n",
    "\n",
    "Now let's concatenate the words with the highest absolute delta sentiment values and use the prior train TSNE embedding model to chart how these words are clustered on the manifold the network has learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'posToNegWordList' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-45851e86e0b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtsneXY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mposToNegWordList\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtsneXY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mposToNegWordList\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mposToNegWordList\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'posToNegWordList' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXkAAAEACAYAAABWLgY0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMSMvSY39H2x3icMl2bwzAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x230992a90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(tsneXY[posToNegWordList, 0], tsneXY[posToNegWordList, 1])\n",
    "\n",
    "for i in posToNegWordList:\n",
    "    ax.annotate(intToWord[i], (tsneXY[i, 0], tsneXY[i, 1]))\n",
    "\n",
    "fig.set_size_inches(25, 25)\n",
    "plt.show()\n",
    "# notice that we have two very distinct clusters in the embedding as one would expect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's import pre-trained word vectors from google and use them to initialize our embedding to see if this improves the neural net's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name sparsetools",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-9f0a77f07eff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#get pre trained word2vec from google:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#https://doc-0k-4g-docs.googleusercontent.com/docs/securesc/gnqvgap6hjncpd3b10i2tv865io48jas/hmjtdgee48c14e1parufukrpkb8urra5/1463018400000/06848720943842814915/09676831593570546402/0B7XkCwpI5KDYNlNUTTlSS21pQmM?e=download&nonce=4l49745nmtine&user=09676831593570546402&hash=i2qe9mshan4mesl112ct9bu1tj9kr1hq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/rodrigogonzalez/anaconda/lib/python2.7/site-packages/gensim/__init__.pyc\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \"\"\"\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mparsing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatutils\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterfaces\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimilarities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummarization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/rodrigogonzalez/anaconda/lib/python2.7/site-packages/gensim/models/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mhdpmodel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHdpModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mldamodel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLdaModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mlsimodel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLsiModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtfidfmodel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTfidfModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mrpmodel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRpModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/rodrigogonzalez/anaconda/lib/python2.7/site-packages/gensim/models/lsimodel.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msparsetools\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minterfaces\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatutils\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name sparsetools"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "#get pre trained word2vec from google:\n",
    "#https://doc-0k-4g-docs.googleusercontent.com/docs/securesc/gnqvgap6hjncpd3b10i2tv865io48jas/hmjtdgee48c14e1parufukrpkb8urra5/1463018400000/06848720943842814915/09676831593570546402/0B7XkCwpI5KDYNlNUTTlSS21pQmM?e=download&nonce=4l49745nmtine&user=09676831593570546402&hash=i2qe9mshan4mesl112ct9bu1tj9kr1hq\n",
    "\n",
    "googlew2v = Word2Vec.load_word2vec_format('./googleword2vec/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.23917413  0.15897956  0.8434785  ...,  0.98806871  0.38205706\n",
      "   0.62928209]\n",
      " [ 0.20818576  0.28512894  0.81496149 ...,  0.10549677  0.66347787\n",
      "   0.5554359 ]\n",
      " [ 0.17786342  0.74596074  0.56105008 ...,  0.03089977  0.24301773\n",
      "   0.1612143 ]\n",
      " ..., \n",
      " [ 0.01059982  0.73107178  0.85167182 ...,  0.10719753  0.24103105\n",
      "   0.96736075]\n",
      " [ 0.58931252  0.06267025  0.33400423 ...,  0.78324782  0.42224119\n",
      "   0.46400115]\n",
      " [ 0.21190755  0.53118813  0.54633112 ...,  0.53822338  0.7123129\n",
      "   0.92488964]]\n",
      "(10000, 300)\n"
     ]
    }
   ],
   "source": [
    "# get word vectors for words in my index\n",
    "googleVecs = []\n",
    "for value in range(max_features):\n",
    "    try:\n",
    "        googleVecs.append(googlew2v[intToWord[value]])\n",
    "    except:\n",
    "        googleVecs.append(np.random.uniform(size=300))\n",
    "\n",
    "googleVecs = np.array(googleVecs)\n",
    "\n",
    "print(googleVecs)\n",
    "print(googleVecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_5 (InputLayer)             (None, 200)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_5 (Embedding)          (None, 200, 300)      3000000     input_5[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_5 (BatchNormal(None, 200, 300)      600         embedding_5[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "lstm_7 (LSTM)                    (None, 64)            93440       batchnormalization_5[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "lstm_8 (LSTM)                    (None, 64)            93440       batchnormalization_5[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "merge_3 (Merge)                  (None, 128)           0           lstm_7[0][0]                     \n",
      "                                                                   lstm_8[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)              (None, 128)           0           merge_3[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_5 (Dense)                  (None, 1)             129         dropout_5[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 3187609\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Bi-directional google\n",
    "\n",
    "# this example tests if using pretrained embeddings will improve performance \n",
    "# relative to starting with random embeddings\n",
    "\n",
    "# this is the placeholder tensor for the input sequences\n",
    "sequence = Input(shape=(max_len,), dtype='int32')\n",
    "# this embedding layer will transform the sequences of integers\n",
    "# into vectors of size embedding\n",
    "# embedding layer converts dense int input to one-hot in real time to save memory\n",
    "embedded = Embedding(max_features, 300, input_length=max_len, weights=[googleVecs])(sequence)\n",
    "# normalize embeddings by input/word in sentence\n",
    "bnorm = BatchNormalization()(embedded)\n",
    "\n",
    "# apply forwards LSTM layer size lstm_neurons\n",
    "forwards = LSTM(lstm_neurons, dropout_W=0.4, dropout_U=0.4)(bnorm)\n",
    "# apply backwards LSTM\n",
    "backwards = LSTM(lstm_neurons, dropout_W=0.4, dropout_U=0.4, go_backwards=True)(bnorm)\n",
    "\n",
    "# concatenate the outputs of the 2 LSTMs\n",
    "merged = merge([forwards, backwards], mode='concat', concat_axis=-1)\n",
    "after_dp = Dropout(0.5)(merged)\n",
    "output = Dense(1, activation='sigmoid')(after_dp)\n",
    "\n",
    "model_bidir_google = Model(input=sequence, output=output)\n",
    "# review model structure\n",
    "print(model_bidir_google.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/6\n",
      "557s - loss: 0.6557 - acc: 0.5998 - val_loss: 0.4919 - val_acc: 0.7657\n",
      "Epoch 2/6\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-5a2115605c03>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m                     \u001b[0mnb_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m                     verbose=2)\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/rodrigogonzalez/anaconda/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight)\u001b[0m\n\u001b[1;32m   1051\u001b[0m                               \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1052\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1053\u001b[0;31m                               callback_metrics=callback_metrics)\n\u001b[0m\u001b[1;32m   1054\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1055\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/rodrigogonzalez/anaconda/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, nb_epoch, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics)\u001b[0m\n\u001b[1;32m    789\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 791\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    792\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    793\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/rodrigogonzalez/anaconda/lib/python2.7/site-packages/keras/backend/theano_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    516\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 518\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/rodrigogonzalez/anaconda/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    898\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 899\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    900\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/rodrigogonzalez/anaconda/lib/python2.7/site-packages/theano/gof/op.pyc\u001b[0m in \u001b[0;36mrval\u001b[0;34m(p, i, o, n)\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNoParams\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m             \u001b[0;31m# default arguments are stored in the closure of `rval`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 904\u001b[0;31m             \u001b[0;32mdef\u001b[0m \u001b[0mrval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode_input_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode_output_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    905\u001b[0m                 \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    906\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Bi-directional google\n",
    "\n",
    "model_bidir_google.compile('rmsprop', 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "start_time = time.time()\n",
    "\n",
    "history_bidir_google = model_bidir_google.fit(X_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    nb_epoch=epochs,\n",
    "                    validation_data=[X_test, y_test], \n",
    "                    verbose=2)\n",
    "\n",
    "end_time = time.time()\n",
    "average_time_per_epoch = (end_time - start_time) / epochs\n",
    "print(\"avg sec per epoch:\", average_time_per_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model with pre-trained word2vec vector's didn't outperform a random embedding, but I suspect it may have if I had run more epochs. Moreover, the number of parameters went up ~3x because the pre-trained vectors were 300d rather than 128d. I suspect I would have had better luck using the glove 6 billion vectors which are 100d instead. Our dataset is just too small to fit a 3 million weight embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test = \"I would want to see this movie.\"\n",
    "\n",
    "test = imdbTokenizer.texts_to_sequences([test])\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "test = sequence.pad_sequences(test, maxlen=max_len)\n",
    "\n",
    "test\n",
    "\n",
    "model_bidir_rmsprop.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
